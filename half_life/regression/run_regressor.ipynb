{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20d5284-43a5-4c57-9006-828f0883a2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "/home/icb/sergey.vilov/miniconda3/envs/mlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import sklearn.pipeline \n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.linear_model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import optuna\n",
    "\n",
    "sys.path.append(\"/home/icb/sergey.vilov/workspace/MLM/mpra/utils/\") \n",
    "\n",
    "from models import *\n",
    "from misc import dotdict\n",
    "\n",
    "import scipy.stats\n",
    "import pickle\n",
    "\n",
    "import gensim.models \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "\n",
    "model_colors = {'Species-aware':\"#E69F00\",'Species-agnostic':\"#56B4E9\",\n",
    "                'DNABERT-2':\"#CC79A7\", 'NT-MS-v2-500M':\"#009E73\", \"k-mers\":\"#0072B2\",\n",
    "               'Saluki human':\"#F0E442\",'BC3MS(k-mers)':\"#0072B2\", 'BC3MS(emb)':\"#D55E00\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e82215c-3d72-4985-acbf-fd521ff920d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/MLM/agarwal_2022/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817e0df3-e2d5-4a5d-a727-ee57f2006ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_df = pd.read_csv(data_dir + 'data/saluki_paper/Fig3_S4/binnedgenes.txt', sep='\\t', usecols=[0,1],\n",
    "                      names=['Fold','gene_id'], skiprows=1).set_index('gene_id') #folds as they are in Agarwal article\n",
    "\n",
    "folds_df = folds_df-1 #to 0-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f664e9a-5b38-4dd2-823b-9548ad26588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_df = pd.read_csv(data_dir + 'data/human/seqFeatWithKmerFreqs.txt.gz', sep='\\t', \n",
    "#                                      usecols=lambda x: not 'ORF.' in x and not '5UTR.' in x).set_index('GENE') #basic features. 3'UTR and 5'UTR k-mers, ORF, target\n",
    "\n",
    "features_df = pd.read_parquet(data_dir + 'data/human/seqFeatWithKmerFreqs_no5UTR.parquet').set_index('GENE')\n",
    "\n",
    "target_df = features_df[['HALFLIFE']]\n",
    "features_df = features_df.drop(columns='HALFLIFE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fefde16d-66de-4d65-b2cf-a43215c540e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_to_gene = pd.read_csv(data_dir + '../UTR_coords/GRCh38_EnsembleCanonical_HGNC.tsv.gz', sep='\\t', \n",
    "                                     names=['gene_id','transcript_id'], skiprows=1,usecols=[0,1]).set_index('transcript_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "102b76cb-4f0f-4056-87eb-65d119529c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get FASTA seqs\n",
    "\n",
    "human_fasta = data_dir + '../fasta/240_species/species/Homo_sapiens.fa'\n",
    "\n",
    "utr_df = defaultdict(str)\n",
    "\n",
    "with open(human_fasta, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            transcript_id = line[1:].split(':')[0].split('.')[0]\n",
    "        else:\n",
    "            utr_df[transcript_id] += line.rstrip().upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7589c703-e38c-4d11-bfec-bc08fbb93361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utr_df = pd.DataFrame(utr_df.values(),\n",
    "             index=transcript_to_gene.loc[utr_df.keys()].gene_id, \n",
    "             columns=['seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62141475-833c-4454-9c27-137e8d7686f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_params = dotdict({})\n",
    "\n",
    "input_params.model_name = '3K' #embedding name, can be \"Species-aware\",\"\"Species-agnostic\", \"MLM\" \"word2vec\" \"griesemer\" or \"Nmers\" where N is an integer\n",
    "\n",
    "input_params.output_dir = './test' #output folder\n",
    "\n",
    "input_params.N_trials = 300 #number of optuna trials\n",
    "input_params.keep_first = True #perform hpp search only at the first split, then use these hyperparameters\n",
    "\n",
    "input_params.N_splits = 100 #number of GroupShuffleSplits\n",
    "input_params.N_CVsplits = 5 #number of CV splits for hyperparameter search\n",
    "input_params.seed = 1 #seed fot GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd9bd8f-7745-402b-b98d-cc2064a81c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3K'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_features = input_params.model_name.split('-')[-1]\n",
    "base_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1babe65-eebf-4b44-80ff-9848eb8659d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding k-mer embeddings for 3'UTRs\n"
     ]
    }
   ],
   "source": [
    "data_df = [folds_df,target_df]\n",
    "\n",
    "if 'B' in base_features:\n",
    "    \n",
    "    print('adding basic features')\n",
    "    data_df.append(features_df.iloc[:,:8])\n",
    "\n",
    "if 'C' in base_features:\n",
    "\n",
    "    print('adding codons')\n",
    "    data_df.append(features_df[[x for x in features_df.columns if x.startswith('Codon.')]])\n",
    "\n",
    "if '3K' in base_features:\n",
    "\n",
    "    print(\"adding k-mer embeddings for 3'UTRs\")\n",
    "    data_df.append(features_df[[x for x in features_df.columns if x.startswith('3UTR.')]])\n",
    "\n",
    "if 'S' in base_features:\n",
    "\n",
    "    print('adding SeqWeaver RBP binding (780)')\n",
    "    for region in ('3pUTR','5pUTR','ORF'):\n",
    "        df = pd.read_csv(data_dir + f'data/human/SeqWeaver_predictions/{region}_avg.txt.gz', sep='\\t').set_index('Group.1')\n",
    "        data_df.append(df)\n",
    "\n",
    "if 'M' in base_features:\n",
    "    print('miRNA target repression (319)')\n",
    "    df = pd.read_csv(data_dir + 'data/human/CWCS.txt.gz', sep='\\t').set_index('GeneID')\n",
    "    data_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aa53c7c-5eaf-4749-8afd-a4da0a73a56a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_df = pd.concat(data_df,axis=1) #concat all features, except embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac4ac09a-ff0b-4bce-bb1c-da784565d88c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_df = data_df[~data_df.HALFLIFE.isna()]\n",
    "data_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89e4b90f-d162-4f9f-9045-a25f6d32670a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#get MLM embeddings\n",
    "\n",
    "mlm_embeddings = []\n",
    "\n",
    "if  input_params.model_name.startswith('Species-'):\n",
    "    embeddings_dir = f'{data_dir}../3UTR_embeddings/{\"-\".join(input_params.model_name.split(\"-\")[:-1])}'\n",
    "    print(f'Loading model embeddings from {embeddings_dir}/embeddings.npy')\n",
    "    mlm_embeddings = np.load(f'{embeddings_dir}/embeddings.npy')\n",
    "    utr_names = pd.read_csv( f'{embeddings_dir}/Homo_sapiens.fa.fai', \n",
    "                            usecols=[0], sep='\\t',names=['seq_name']).seq_name.apply(lambda x:x.split(':')[0].split('.')[0]).values \n",
    "\n",
    "elif  input_params.model_name.startswith('NT-') or  input_params.model_name.startswith('DNABERT-2'):\n",
    "    embeddings_dir = f'{data_dir}../3UTR_embeddings/{\"-\".join(input_params.model_name.split(\"-\")[:-1])}'\n",
    "    print(f'Loading model embeddings from {embeddings_dir}')\n",
    "    mlm_embeddings, utr_names = [], []\n",
    "    for emb_file in tqdm(glob(f'{embeddings_dir}/ENST*.pickle')):\n",
    "        with open(emb_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            utr_names_batch, embeddings_batch = data[0],data[1]\n",
    "            mlm_embeddings.extend(embeddings_batch)\n",
    "            utr_names.extend(utr_names_batch)\n",
    "    mlm_embeddings = np.vstack(mlm_embeddings)\n",
    "    utr_names = [x.split('.')[0] for x in utr_names]\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90ca9f16-a4a4-49fd-a1a5-6a32e1b43a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get sequence embedding depending on the model\n",
    "\n",
    "if len(mlm_embeddings)>0:            \n",
    "\n",
    "    #MLM embeddings are made for transcripts\n",
    "    #get corresponding gene names\n",
    "\n",
    "    embedding_transcripts = [x.split('.')[0] for x in utr_names]\n",
    "\n",
    "    embeddings_df = pd.DataFrame(mlm_embeddings, \n",
    "                                     index=transcript_to_gene.loc[embedding_transcripts].gene_id, \n",
    "                                     columns=[f'emb_{x}' for x in range(mlm_embeddings.shape[1])])\n",
    "\n",
    "elif 'mers' in input_params.model_name:\n",
    "    \n",
    "    k = int(input_params.model_name[0])\n",
    "        \n",
    "    kmerizer = Kmerizer(k=k)\n",
    "    \n",
    "    Nmer_embeddings = utr_df.seq.apply(lambda x: kmerizer.kmerize(x))\n",
    "    \n",
    "    embeddings_df = pd.DataFrame(Nmer_embeddings.tolist(), index=Nmer_embeddings.index, columns=[f'emb_{x}' for x in range(4**k)])\n",
    "\n",
    "elif input_params.model_name=='word2vec':\n",
    "        \n",
    "    kmerizer_w2v = Kmerizer(k=4)\n",
    "\n",
    "    w2v_model = gensim.models.Word2Vec(sentences=utr_df.seq.apply(lambda x: kmerizer_w2v.tokenize(x)), \n",
    "                             vector_size=128, window=5, min_count=1, workers=4, sg=1) #default: CBOW\n",
    "\n",
    "    word2vec_emb = utr_df.seq.apply(\n",
    "        lambda x: np.mean([w2v_model.wv[x]  for x in kmerizer_w2v.tokenize(x)],axis=0)) #average embedding of all 4-mers in the sequence\n",
    "\n",
    "    word2vec_emb = word2vec_emb[~word2vec_emb.isna()]\n",
    "    \n",
    "    embeddings_df = pd.DataFrame(word2vec_emb.tolist(), index=word2vec_emb.index, columns=[f'emb_{x}' for x in range(128)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dff2e12f-c719-4788-96f5-c5a2a9ba2981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(mlm_embeddings)>0:            \n",
    "\n",
    "    data_df = pd.concat([data_df,embeddings_df], join='inner', axis=1)\n",
    "\n",
    "#data_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b97eeb67-7133-4ead-bcb9-b72b0749164c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data_df.drop(columns=['Fold','HALFLIFE']).values#all columns except HALFLIFE and fold\n",
    "\n",
    "y = data_df['HALFLIFE'].values\n",
    "\n",
    "genes = data_df.index.values\n",
    "\n",
    "folds = data_df['Fold'].values\n",
    "\n",
    "N_folds = int(max(folds))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19754749-4f8c-4eec-94d9-08566d46cefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fc5beaf-d2ee-45ad-a40f-309609edff82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_SVR(args):\n",
    "        \n",
    "    test_hpp, (train_idx, test_idx) = args \n",
    "\n",
    "    pipe = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(),\n",
    "                                                  sklearn.svm.SVR(**test_hpp))\n",
    "    pipe.fit(X[val_idx][train_idx],y[val_idx][train_idx])\n",
    "\n",
    "    R2_score = pipe.score(X[val_idx][test_idx],y[val_idx][test_idx])\n",
    "        \n",
    "    return R2_score\n",
    "\n",
    "\n",
    "def hpp_search(val_idx, cv_splits = 10):\n",
    "    \n",
    "    '''\n",
    "    Perform Hyperparameter Search using OPTUNA Bayesian Optimisation strategy\n",
    "    \n",
    "    The bets hyperparameters should maximize coefficient of determination (R2)\n",
    "    \n",
    "    The hyperparameter range should first be adjused with grid search to make the BO algorithm converge in reasonable time\n",
    "    '''\n",
    "\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        C = trial.suggest_float(\"C\", 1e-2, 1e2, log=True)\n",
    "        epsilon = trial.suggest_float(\"epsilon\", 1e-5, 1, log=True)\n",
    "        gamma = trial.suggest_float(\"gamma\", 1e-5, 1, log=True)\n",
    "        \n",
    "        test_hpp = {'C':C, 'epsilon':epsilon, 'gamma':gamma}\n",
    "        \n",
    "        pool = Pool(processes=input_params.n_jobs,maxtasksperchild=3)\n",
    "\n",
    "        cv_scores = []\n",
    "        \n",
    "        kfold = sklearn.model_selection.KFold(n_splits=cv_splits)\n",
    "        \n",
    "        params = zip((test_hpp for fold_idx in range(cv_splits)), kfold.split(X[val_idx], y[val_idx]))\n",
    "        \n",
    "        for res in pool.imap(apply_SVR,params):\n",
    "            cv_scores.append(res)\n",
    "     \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    \n",
    "        return np.mean(cv_scores)\n",
    "    \n",
    "    study = optuna.create_study(direction = \"maximize\")\n",
    "\n",
    "    study.optimize(objective, n_trials = input_params.N_trials)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd15461-1efb-4ad0-b8c5-f11efba5760a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "cv_scores = [] #scores and best hyperparameters for each split\n",
    "res_df = []\n",
    "\n",
    "best_hpp_models = {'Species-aware-BC3MS':{'C': 17.2, 'epsilon': 0.73, 'gamma': 2e-4},\n",
    "            'Species-agnostic-BC3MS':{'C': 15.3, 'epsilon': 0.37, 'gamma': 1.8e-4},\n",
    "            'NT-MS-v2-500M-BC3MS':{'C': 100, 'epsilon': 0.84, 'gamma': 1.3e-5},\n",
    "            'DNABERT-2-BC3MS':{'C': 19, 'epsilon': 9.8e-5, 'gamma': 1.4e-4},\n",
    "            'DNABERT-2-B3': {'C': 100, 'epsilon': 2e-3, 'gamma': 6.6e-05},\n",
    "            'BCMS': {'C': 18.5, 'epsilon': 2.8e-2, 'gamma': 1.3e-4},\n",
    "            'Species-agnostic-B3':{'C': 86, 'epsilon': 0.06, 'gamma': 2.4e-4},\n",
    "            'Species-agnostic-3':{'C': 22, 'epsilon': 0.003, 'gamma': 5.6e-4},\n",
    "            'Species-aware-3':{'C': 2.05, 'epsilon': 0.091, 'gamma': 0.0037},\n",
    "            '3K': {'C': 8.7, 'epsilon': 1.8e-3, 'gamma': 4.2e-05},\n",
    "            'DNABERT-2-3': {'C': 0.69, 'epsilon': 2.6e-4, 'gamma': 6.3e-4},\n",
    "            'NT-MS-v2-500M-3':{'C': 21, 'epsilon': 0.75, 'gamma': 3.8e-05},\n",
    "           }\n",
    "\n",
    "for fold in range(N_folds):\n",
    "    \n",
    "        print(f'Fold {fold}')\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = X[folds!=fold],X[folds==fold],y[folds!=fold],y[folds==fold]\n",
    "\n",
    "        if input_params.model_name not in best_hpp_models.keys() and (fold==0 or input_params.keep_first==False):\n",
    "            #perform only ones if input_params.keep_first==True\n",
    "            val_idx = np.where(folds==0)[0]\n",
    "            best_hpp = hpp_search(val_idx,cv_splits = input_params.N_CVsplits)\n",
    "        elif input_params.model_name  in best_hpp_models.keys():\n",
    "            best_hpp = best_hpp_models[input_params.model_name]\n",
    "            \n",
    "        pipe = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(),\n",
    "                                              sklearn.svm.SVR(**best_hpp))\n",
    "        \n",
    "        pipe.fit(X_train,y_train)\n",
    "                    \n",
    "        y_pred = pipe.predict(X_test) \n",
    "                \n",
    "        #pipe_lasso = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(), sklearn.linear_model.LassoCV(cv=input_params.N_CVsplits, alphas=10.**np.arange(-6,0))) \n",
    "        #pipe_lasso.fit(X_train,y_train)\n",
    "        #y_pred_lasso = pipe_lasso.predict(X_test)\n",
    "\n",
    "        pipe_ridge = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(), sklearn.linear_model.RidgeCV(cv=input_params.N_CVsplits, alphas=10.**np.arange(-5,5))) \n",
    "        pipe_ridge.fit(X_train,y_train)\n",
    "        y_pred_ridge = pipe_ridge.predict(X_test)\n",
    "        \n",
    "        fold_res = np.vstack([np.ones((len(y_test),))*fold,genes[folds==fold],y_test,y_pred_ridge,y_pred]).T\n",
    "\n",
    "        res_df.append(fold_res)\n",
    "        #cv_scores.append({'fold':fold,\n",
    "        #                 'r2_svr':sklearn.metrics.r2_score(y_test,y_pred),\n",
    "        #                'pearson_r_svr':scipy.stats.pearsonr(y_test,y_pred)[0],\n",
    "        #                 'r2_ridge':sklearn.metrics.r2_score(y_test,y_pred_ridge),\n",
    "        #                 'pearson_r_ridge':scipy.stats.pearsonr(y_test,y_pred_ridge)[0]\n",
    "        #                 }|best_hpp)\n",
    "\n",
    "    \n",
    "#cv_scores = pd.DataFrame(cv_scores)\n",
    "res_df = np.vstack(res_df)\n",
    "res_df = pd.DataFrame(res_df,columns=['fold','gene','y_true','y_pred_ridge','y_pred_svr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93970bc4-68fc-4209-989f-c796a1728cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cv_scores.to_csv(data_dir + f'predictions/{input_params.model_name}.tsv', sep = '\\t', index=None)\n",
    "res_df.to_csv(data_dir + f'predictions/{input_params.model_name}-full.tsv', sep = '\\t', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
