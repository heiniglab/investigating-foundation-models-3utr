{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f78039f-8dd7-43d1-8ac6-684736938326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import argparse\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pysam\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from transformers import PreTrainedTokenizerFast, DataCollatorForLanguageModeling\n",
    "\n",
    "from DNABERT2.configuration_bert import BertConfig\n",
    "from DNABERT2.bert_layers import BertForMaskedLM\n",
    "\n",
    "import helpers.misc as misc                #miscellaneous functions\n",
    "import helpers.train_eval as train_eval    #train and evaluation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05dcdca1-c81a-41fc-a8ed-40ee96842412",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eca49b9-8937-4ef5-8291-d119c037f254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA device: Tesla V100S-PCIE-32GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'\\nCUDA device: {torch.cuda.get_device_name(0)}\\n')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('\\nCUDA device: CPU\\n')\n",
    "    #raise Exception('CUDA is not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc332645-382d-4109-812a-938dac8eb8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad2f7d28-d069-45bc-a6dc-f5834ee63f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, fasta_fa, seq_df):\n",
    "        \n",
    "        if fasta_fa:\n",
    "            self.fasta = pysam.FastaFile(fasta_fa)\n",
    "        else:\n",
    "             self.fasta = None\n",
    "\n",
    "        self.seq_df = seq_df\n",
    "        self.start = 0\n",
    "        self.end = len(self.seq_df)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.seq_df)\n",
    "                \n",
    "    def __iter__(self):\n",
    "        \n",
    "        #worker_total_num = torch.utils.data.get_worker_info().num_workers\n",
    "        #worker_id = torch.utils.data.get_worker_info().id\n",
    "        \n",
    "        for seq_idx in range(self.start,self.end):\n",
    "            \n",
    "            if self.fasta:\n",
    "                seq = self.fasta.fetch(self.seq_df.iloc[seq_idx].seq_name).upper()\n",
    "            else:\n",
    "                seq = self.seq_df.iloc[seq_idx].seq.upper()\n",
    "    \n",
    "            #species_label = self.seq_df.iloc[idx].species_label\n",
    "            \n",
    "            seq = seq.replace('-','')\n",
    "\n",
    "            tokenized_seq = tokenizer(seq, add_special_tokens=False)['input_ids']\n",
    "\n",
    "            #N_tokens_overlap=np.random.randint(low=0,high=input_params.max_overlap_tokens)\n",
    "\n",
    "            tokenized_chunks, _ = misc.get_chunks(tokenized_seq, \n",
    "                                                   N_tokens_chunk=input_params.max_tokens, \n",
    "                                                   N_tokens_overlap=input_params.max_overlap_tokens,\n",
    "                                                   tokenizer_cls_token_id=tokenizer.cls_token_id,\n",
    "                                                   tokenizer_eos_token_id=tokenizer.sep_token_id,\n",
    "                                                   tokenizer_pad_token_id=tokenizer.pad_token_id,\n",
    "                                                   padding=True)\n",
    "\n",
    "            for tokenized_chunk in tokenized_chunks:\n",
    "\n",
    "                attention_mask = [1 if token_id!=tokenizer.pad_token_id else 0 for token_id in tokenized_chunk]\n",
    "                \n",
    "                tokenized_chunk = {'input_ids':tokenized_chunk,\n",
    "                                   'seq_idx':seq_idx,\n",
    "                                   'token_type_ids':[0]*len(tokenized_chunk), \n",
    "                                   'attention_mask':attention_mask}\n",
    "                \n",
    "                yield tokenized_chunk\n",
    "                        \n",
    "    def close(self):\n",
    "        self.fasta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "585b6abb-2ac1-4da9-929c-8163e67e7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_params = misc.dotdict({})\n",
    "\n",
    "input_params.max_overlap_tokens = 25\n",
    "input_params.max_tokens = 127\n",
    " \n",
    "input_params.train_chunks = 8\n",
    "input_params.batch_size = 16\n",
    "input_params.weight_decay = 1e-5\n",
    "input_params.max_lr = 1e-4\n",
    " \n",
    "input_params.fasta = datadir + 'fasta/241_mammals.shuffled.fa'\n",
    "#input_params.fasta = datadir + 'fasta/Homo_sapiens_rna.fa'\n",
    "#input_params.fasta = datadir + 'fasta/Homo_sapiens_dna_fwd.fa'\n",
    "\n",
    "input_params.output_dir = './test/'\n",
    "input_params.tot_epochs = 10\n",
    "input_params.val_fraction = 0.1\n",
    "input_params.validate_every = 1\n",
    "input_params.save_at = [2]\n",
    "input_params.save_at = ['-1']\n",
    "\n",
    "input_params.save_at = misc.list2range(input_params.save_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fc2d9fd3-c2a0-4a5f-ba36-bf0f71b9aa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [02:54, ?it/s]\n",
      "acc: 0.000343, masked acc: 0.00068, loss: 8.439:   0%|█                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 752/425668 [01:45<16:29:31,  7.16it/s]\n",
      "acc: 0.000642, masked acc: 0.000783, loss: 8.441:   0%|▏                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 164/425668 [01:32<66:24:51,  1.78it/s]\n",
      "acc: 0.00063, masked acc: 0.000428, loss: 8.439:   0%|▏                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 164/425668 [01:26<61:59:44,  1.91it/s]\n",
      "acc: 0.000636, masked acc: 0.000867, loss: 8.448:   0%|▏                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 139/425668 [01:21<69:14:11,  1.71it/s]\n",
      "acc: 0.000621, masked acc: 0.000969, loss: 8.442:   0%|▏                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 139/425668 [01:16<64:44:00,  1.83it/s]\n",
      "acc: 0.000606, masked acc: 0.000617, loss: 8.437:   0%|▎                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 246/425668 [00:58<27:55:15,  4.23it/s]\n"
     ]
    }
   ],
   "source": [
    "seq_df = pd.read_csv(input_params.fasta + '.fai', header=None, sep='\\t', usecols=[0,1], names=['seq_name','seq_len'])\n",
    "\n",
    "#seq_df = seq_df.iloc[:3000]\n",
    "\n",
    "#seq_df['n_chunks'] = seq_df.seq_len.apply(lambda x:np.ceil(x / (input_params.max_tokens-2-input_params.overlap_bp))).astype(int)\n",
    "\n",
    "seq_df['species_name'] = seq_df.seq_name.apply(lambda x:x.split(':')).apply(lambda x:x[1] if len(x)==2 else 'Homo_sapiens')\n",
    "#seq_df['species_name']  = 'Homo_sapiens'\n",
    "\n",
    "all_species = sorted(seq_df.species_name.unique())\n",
    "\n",
    "if not input_params.species_agnostic:\n",
    "    species_encoding = {species:idx for idx,species in enumerate(all_species)}\n",
    "else:\n",
    "    species_encoding = {species:0 for species in all_species}\n",
    "    \n",
    "seq_df['species_label'] = seq_df.species_name.map(species_encoding)\n",
    "\n",
    "#seq_df = seq_df.sample(frac = 1., random_state = 1) #DO NOT SHUFFLE, otherwise too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ccadd11f-f0f0-42be-b7b7-be9a6f228650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = [\"NNNNNN\"]\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"./DNABERT2/tokenizer.json\",\n",
    "mask_token = '[MASK]', pad_token = '[PAD]', sep_token = '[SEP]', cls_token = '[CLS]', unk_token = '[UNK]',)\n",
    "tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "62d8b24e-bff6-4fd3-917a-31108ff43c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(4097, 768, padding_idx=0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained('./DNABERT2/config.json')\n",
    "\n",
    "config.vocab_size = config.vocab_size + len(new_tokens)\n",
    "\n",
    "config.max_position_embeddings = max(input_params.max_tokens,512)\n",
    "config.alibi_starting_size = max(input_params.max_tokens,512)\n",
    "\n",
    "model = BertForMaskedLM(config).to(device)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "90f7f26f-2c20-4762-b948-8753c05e34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dir = './DNABERT2/'\n",
    "#model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/dnabert2-3utr/checkpoints/epoch_18/'\n",
    "#\n",
    "#tokenizer = PreTrainedTokenizerFast(tokenizer_file=model_dir + \"tokenizer.json\",\n",
    "#mask_token = '[MASK]', pad_token = '[PAD]', sep_token = '[SEP]', cls_token = '[CLS]', unk_token = '[UNK]',)\n",
    "#config = BertConfig.from_pretrained(model_dir + 'config.json')\n",
    "#model = BertForMaskedLM(config).to(device)\n",
    "#\n",
    "#collate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "#model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/dnabert2-3utr-old/dnabert2-3utr/checkpoints/epoch_20/'\n",
    "\n",
    "#test_dataset = SeqDataset(input_params.fasta, seq_df)\n",
    "#test_dataloader = DataLoader(dataset = test_dataset, batch_size = 64, \n",
    "#                             num_workers = 1,  worker_init_fn=misc.worker_init_fn, collate_fn = collate_fn, shuffle = False)\n",
    "#\n",
    "#val_metrics =  train_eval.model_eval(model, None, test_dataloader, device,\n",
    "#                    silent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d4efdc8a-ca30-4b5e-8b8c-14bcf5f0fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "39ac0b02-19e7-476b-bed8-0a3d3ecb8caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73813/3011938391.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['train_chunk'] = train_chunk[:N_train]\n"
     ]
    }
   ],
   "source": [
    "N_train = int(len(seq_df)*(1-input_params.val_fraction))       \n",
    "train_df, test_df = seq_df.iloc[:N_train], seq_df.iloc[N_train:]\n",
    "\n",
    "train_chunk = np.repeat(list(range(input_params.train_chunks)),repeats = N_train // input_params.train_chunks + 1 )\n",
    "train_df['train_chunk'] = train_chunk[:N_train]\n",
    "\n",
    "train_dataset = SeqDataset(input_params.fasta, train_df)\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size = input_params.batch_size, \n",
    "                              num_workers = 1, worker_init_fn=misc.worker_init_fn, collate_fn = collate_fn, shuffle = False)\n",
    "\n",
    "test_dataset = SeqDataset(input_params.fasta, test_df)\n",
    "test_dataloader = DataLoader(dataset = test_dataset, batch_size = input_params.batch_size, \n",
    "                             num_workers = 1,  worker_init_fn=misc.worker_init_fn, collate_fn = collate_fn, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4c1a6441-7bef-46f4-a00f-3ef815883b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=4\n",
    "test_dataset.seq_df = train_df[train_df.train_chunk == (epoch-1) % input_params.train_chunks]\n",
    "test_dataset.end = len(test_dataset.seq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e432d677-043f-4e29-af7f-756205364242",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset = test_dataset, batch_size = 256, \n",
    "                             num_workers = 1,  worker_init_fn=misc.worker_init_fn, collate_fn = collate_fn, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a13e2b-9e8e-4f3d-b6a9-0bfcf95d0325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928a259ffe29441691efa638ee3ba78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "val_metrics =  train_eval.model_eval(model, None, test_dataloader, device,\n",
    "                    silent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c1ef4-0137-45f6-828a-cc98b9678956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer\n",
    "#\n",
    "#pretrained_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/MLM/dnabert2/DNABERT-2-117M/'\n",
    "#tokenizer = AutoTokenizer.from_pretrained(pretrained_dir,trust_remote_code=True)\n",
    "#model = BertForMaskedLM.from_pretrained(pretrained_dir).to(device)\n",
    "#\n",
    "#val_metrics =  train_eval.model_eval(model, None, test_dataloader, device,\n",
    "#                    silent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9d8cb44-dcf9-4576-9043-9977b4428047",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61c565b1-7538-4620-9e5f-0c7d85aa24e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_params, betas=(0.9,0.98), eps=1e-6,\n",
    "                             lr = input_params.max_lr, \n",
    "                             weight_decay = input_params.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4417aaf-3eea-4133-9c0c-714c1f34d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_params.checkpoint_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/dnabert2-3utr/single_gpu/checkpoints/epoch_4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6781f87-caed-4b64-bd2b-19c3bb338a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr = 0, max_lr = input_params.max_lr,\n",
    "                                             step_size_up = 30000, step_size_down = 470000, cycle_momentum=False,\n",
    "                                             last_epoch = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68f27a3a-feca-4a2c-b37f-5e925c38180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = 0\n",
    "\n",
    "if input_params.checkpoint_dir:\n",
    "\n",
    "    model = BertForMaskedLM.from_pretrained(input_params.checkpoint_dir).to(device)\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(input_params.checkpoint_dir)\n",
    "\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(input_params.checkpoint_dir)\n",
    "\n",
    "    if os.path.isfile(input_params.checkpoint_dir + '/optimizer.pt'):\n",
    "            optimizer.load_state_dict(torch.load(input_params.checkpoint_dir + '/optimizer.pt'))\n",
    "            scheduler.load_state_dict(torch.load(input_params.checkpoint_dir + '/scheduler.pt'))\n",
    "\n",
    "    last_epoch = int(input_params.checkpoint_dir.rstrip('/').split('_')[-1]) #infer previous epoch from input_params.checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c028918-7ec8-44a1-b41b-b41a77ee1440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_str(metrics):\n",
    "    loss, total_acc, masked_acc = metrics\n",
    "    return f'loss: {loss:.4}, total acc: {total_acc:.3f}, masked acc: {masked_acc:.3f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce94ff3d-89f2-46f1-ae69-da2d6f4521f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: Training...\n",
      "using train samples: [0, 425667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.000153, masked acc: 0.00104, loss: 8.257:   1%|████▋                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 3301/425668 [03:13<6:53:41, 17.02it/s]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 19.50 GiB total capacity; 17.54 GiB already allocated; 281.88 MiB free; 18.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mseq_df)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musing train samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mseq_df\u001b[38;5;241m.\u001b[39mindex[[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - train (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler\u001b[38;5;241m.\u001b[39mlast_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m iterations), \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_to_str(train_metrics)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m input_params\u001b[38;5;241m.\u001b[39msave_at \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m input_params\u001b[38;5;241m.\u001b[39msave_at: \u001b[38;5;66;03m#save model weights\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/MLM/models/dnabert2-3utr/helpers/train_eval.py:35\u001b[0m, in \u001b[0;36mmodel_train\u001b[0;34m(model, optimizer, dataloader, device, scheduler, silent)\u001b[0m\n\u001b[1;32m     32\u001b[0m targets_masked \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#targets = inputs[\"targets\"].to(device)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_type_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrans/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/MLM/models/dnabert2-3utr/DNABERT2/bert_layers.py:746\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    742\u001b[0m     masked_tokens_mask \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    744\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 746\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasked_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasked_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    762\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrans/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/MLM/models/dnabert2-3utr/DNABERT2/bert_layers.py:611\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, output_all_encoded_layers, masked_tokens_mask, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m     first_col_mask[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    609\u001b[0m     subset_mask \u001b[38;5;241m=\u001b[39m masked_tokens_mask \u001b[38;5;241m|\u001b[39m first_col_mask\n\u001b[0;32m--> 611\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_all_encoded_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_encoded_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m masked_tokens_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrans/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/MLM/models/dnabert2-3utr/DNABERT2/bert_layers.py:467\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers, subset_mask)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    466\u001b[0m     layer_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer[i]\n\u001b[0;32m--> 467\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi_attn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_all_encoded_layers:\n\u001b[1;32m    475\u001b[0m         all_encoder_layers\u001b[38;5;241m.\u001b[39mappend(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrans/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/MLM/models/dnabert2-3utr/DNABERT2/bert_layers.py:330\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, cu_seqlens, seqlen, subset_idx, indices, attn_mask, bias)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    310\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m     bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    317\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass for a BERT layer, including both attention and MLP.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m        bias: None or (batch, heads, max_seqlen_in_batch, max_seqlen_in_batch)\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43msubset_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(attention_output)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrans/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/MLM/models/dnabert2-3utr/DNABERT2/bert_layers.py:243\u001b[0m, in \u001b[0;36mBertUnpadAttention.forward\u001b[0;34m(self, input_tensor, cu_seqlens, max_s, subset_idx, indices, attn_mask, bias)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    223\u001b[0m     input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass for scaled self-attention without padding.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m        bias: None or (batch, heads, max_seqlen_in_batch, max_seqlen_in_batch)\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subset_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(index_first_axis(self_output, subset_idx),\n\u001b[1;32m    247\u001b[0m                            index_first_axis(input_tensor, subset_idx))\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrans/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/MLM/models/dnabert2-3utr/DNABERT2/bert_layers.py:186\u001b[0m, in \u001b[0;36mBertUnpadSelfAttention.forward\u001b[0;34m(self, hidden_states, cu_seqlens, max_seqlen_in_batch, indices, attn_mask, bias)\u001b[0m\n\u001b[1;32m    184\u001b[0m     attention \u001b[38;5;241m=\u001b[39m flash_attn_qkvpacked_func(qkv, bias)\n\u001b[1;32m    185\u001b[0m     attention \u001b[38;5;241m=\u001b[39m attention\u001b[38;5;241m.\u001b[39mto(orig_dtype)\n\u001b[0;32m--> 186\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     attention \u001b[38;5;241m=\u001b[39m flash_attn_qkvpacked_func(qkv, bias)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 19.50 GiB total capacity; 17.54 GiB already allocated; 281.88 MiB free; 18.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()\n",
    "\n",
    "#from utils.misc import print    #print function that displays time\n",
    "\n",
    "if not input_params.test:\n",
    "\n",
    "    for epoch in range(last_epoch+1, input_params.tot_epochs+1):\n",
    "\n",
    "        print(f'EPOCH {epoch}: Training...')\n",
    "\n",
    "        train_dataset.seq_df = train_df[train_df.train_chunk == (epoch-1) % input_params.train_chunks]\n",
    "        train_dataset.end = len(train_dataset.seq_df)\n",
    "        \n",
    "        print(f'using train samples: {list(train_dataset.seq_df.index[[0,-1]])}')\n",
    "\n",
    "        train_metrics = train_eval.model_train(model, optimizer, train_dataloader, device, scheduler=scheduler,\n",
    "                            silent = False)\n",
    "        \n",
    "        print(f'epoch {epoch} - train ({scheduler.last_epoch+1} iterations), {metrics_to_str(train_metrics)}')\n",
    "\n",
    "        if epoch in input_params.save_at or -1 in input_params.save_at: #save model weights\n",
    "\n",
    "            output_dir = misc.save_model_weights(model, optimizer, scheduler, weights_dir, epoch, input_params.save_at)\n",
    "            _ = os.system('cp ./DNABERT2/*.py ' + output_dir) \n",
    "            \n",
    "        if input_params.val_fraction>0 and ( epoch==input_params.tot_epochs or\n",
    "                            (input_params.validate_every and epoch%input_params.validate_every==0)):\n",
    "\n",
    "            print(f'EPOCH {epoch}: Validating...')\n",
    "\n",
    "            val_metrics =  train_eval.model_eval(model, optimizer, test_dataloader, device,\n",
    "                    silent = False)\n",
    "\n",
    "            print(f'epoch {epoch} - validation, {metrics_to_str(val_metrics)}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96752e6-58d2-4470-a25d-220fe74bfc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0dece-39a4-41d8-8c10-e8b3c6c813b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
