{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef155c-efca-43c0-8f0f-e18f33571886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a21ffb4-6ba1-4007-b1e9-68d929b28157",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "N_folds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5dfd26-e6bc-4a6a-af42-c7f8e4c59867",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/MLM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7247644-bed7-424d-a4e8-146bcba7d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = data_dir + 'nucleotide-transform/nucleotide-transformer-v2-500m-multi-species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f6c62f-6d18-4c7b-a55a-4577bbb55362",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8df946-4917-441b-ad30-e37c261ce89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir,trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_dir,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad449bd-5492-4082-8c94-22afb9c510a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(seq, max_seq_length):\n",
    "    '''\n",
    "    Generate a batch by consequently masking every 6-mer in a sequence in a rolling mask fashion\n",
    "    '''\n",
    "\n",
    "    max_tokenized_length = max_seq_length-(max_seq_length//6)*5 #maximum length of the tokenized sequence without 'N's\n",
    "    \n",
    "    seq_token_ids = tokenizer.encode_plus(seq, return_tensors=\"pt\", padding=\"max_length\", max_length = max_tokenized_length)[\"input_ids\"].squeeze()\n",
    "    \n",
    "    #mask_id = tokenizer.token_to_id('<mask>')\n",
    "    #pad_id = tokenizer.token_to_id('<pad>')\n",
    "    \n",
    "    batch_token_masked = []\n",
    "    \n",
    "    for mask_pos in range(1,max_tokenized_length+1):\n",
    "        if seq_token_ids[mask_pos] == tokenizer.pad_token_id:\n",
    "            break\n",
    "        masked_seq = seq_token_ids.clone()\n",
    "        masked_seq[mask_pos] = tokenizer.mask_token_id\n",
    "        batch_token_masked.append(masked_seq)\n",
    "    \n",
    "    batch_token_masked = torch.stack(batch_token_masked)\n",
    "\n",
    "    seq_token_ids = seq_token_ids.numpy()\n",
    "\n",
    "    return seq_token_ids, batch_token_masked #unmasked tokens for the sequence, batch of masked positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ddb5a-7eee-4fee-9d43-22ac1ed986f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_batch(seq_token_ids, batch_token_ids):\n",
    "    '''\n",
    "    Predict on a batch corresponding to a single sequence\n",
    "    '''\n",
    "\n",
    "    with torch.no_grad():\n",
    "        attention_mask = batch_token_ids != tokenizer.pad_token_id   \n",
    "        torch_outs = model(\n",
    "        batch_token_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        encoder_attention_mask=attention_mask,\n",
    "        output_hidden_states=False)\n",
    "\n",
    "    logits = torch_outs[\"logits\"] #max_tokenized_length x (max_tokenized_length+1) x N_tokens\n",
    "\n",
    "    probas = F.softmax(logits, dim=-1).numpy()\n",
    "\n",
    "    seq_probas = []\n",
    "    \n",
    "    for masked_pos, gt_token_id in enumerate(seq_token_ids[1:]): #loop over tokens of unmasked sequence\n",
    "        gt_token = tokenizer.id_to_token(gt_token_id)\n",
    "        if gt_token=='<pad>':\n",
    "            break\n",
    "        assert batch_token_masked[masked_pos,masked_pos+1]==2 #masked position\n",
    "        for idx in range(len(gt_token)):\n",
    "            position_probas = [] #probabilities for all bases at given position\n",
    "            for nuc in 'ACGT':\n",
    "                position_probas.append(probas[masked_pos,masked_pos+1][tokendict_list[idx][nuc]].sum()) #sum over all takens that have given letter at given position\n",
    "            seq_probas.append(position_probas)\n",
    "    \n",
    "    seq_probas = np.array(seq_probas)\n",
    "\n",
    "    return seq_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd367b8-dbec-40ef-82a2-c1ac6484734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_complement(seq):\n",
    "    '''\n",
    "    Take sequence reverse complement\n",
    "    '''\n",
    "    compl_dict = {'A':'T', 'C':'G', 'G':'C', 'T':'A'}\n",
    "    compl_seq = ''.join([compl_dict.get(x,x) for x in seq])\n",
    "    rev_seq = compl_seq[::-1]\n",
    "    return rev_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b87d9-b877-4898-90bb-de51e5266dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokendict_list = [{\"A\": [], \"G\": [], \"T\": [],\"C\": []} for x in range(6)]\n",
    "\n",
    "for tpl in itertools.product(\"ACGT\",repeat=6):\n",
    "    encoding = tokenizer.encode(\"\".join(tpl))\n",
    "    for idx, nuc in enumerate(tpl):\n",
    "        tokendict_list[idx][nuc].append(encoding[1]) #token indices for idx position in 6-mer and letter nuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c379bdd-4ba4-4e77-bcaf-744ad85ab27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(seq,chunk_size):\n",
    "    '''\n",
    "    Chunk the given sequence into chunks of chunk_size\n",
    "    The last chunk is padded with the previous chunk if it's shorter than chunk_size\n",
    "    '''\n",
    "    chunks = [seq[start:start+chunk_size] for start in range(0,len(seq),chunk_size)]\n",
    "    assert ''.join(chunks)==seq\n",
    "    if len(chunks)>1:\n",
    "        pad_length_last = min(chunk_size-len(chunks[-1]), len(chunks[-2]))\n",
    "        if pad_length_last>0:\n",
    "            pad_seq = chunks[-2][-pad_length_last:]\n",
    "            chunks[-1] = pad_seq + chunks[-1]\n",
    "    else:\n",
    "        pad_length_last = 0\n",
    "    assert ''.join([x for x in chunks[:-1]]+[chunks[-1][pad_length_last:]])==seq\n",
    "    return (chunks,pad_length_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e93f9f3b-96b5-4b28-a154-55e1421e8d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 454 sequences\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(data_dir + 'motif_predictions/split_75_25/test.csv')\n",
    "\n",
    "folds = np.arange(N_folds).repeat(len(dataset)//N_folds+1)[:len(dataset)] \n",
    "\n",
    "dataset = dataset.loc[folds==fold]\n",
    "\n",
    "print(f'Fold {fold}: {len(dataset)} sequences')\n",
    "\n",
    "strand_info = pd.read_csv(data_dir + 'UTR_coords/GRCh38_3_prime_UTR_clean-sorted.bed', sep='\\t', header = None, names=['seq_name','strand'], usecols=[3,5]).set_index('seq_name').squeeze()\n",
    "\n",
    "dataset.seq = dataset.apply(lambda x: reverse_complement(x.seq) if strand_info.loc[x.seq_name]=='-' else x.seq, axis=1) #undo reverse complement\n",
    "\n",
    "dataset = dataset.set_index('seq_name').seq\n",
    "\n",
    "dataset = dataset.apply(lambda x:get_chunks(x,chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14ab7538-12e4-4c09-a681-b2710ef958ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for ENST00000316554.5_utr3_0_0_chr12_48184708_f, 2 chunks\n",
      "CPU times: user 19min 31s, sys: 1min 45s, total: 21min 16s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "\n",
    "for seq_name, (seq_chunks,pad_length_last) in dataset.items():\n",
    "    print(f'Predicting for {seq_name}, {len(seq_chunks)} chunks')\n",
    "    seq_probas = []\n",
    "    for seq in seq_chunks:\n",
    "        seq_token_ids, batch_token_masked = generate_batch(seq,chunk_size)\n",
    "        seq_probas.append(predict_on_batch(seq_token_ids, batch_token_masked))\n",
    "    seq_probas[-1] = seq_probas[-1][pad_length_last:] #skip the part used for padding from the previous chunk\n",
    "    seq_probas = np.vstack(seq_probas)\n",
    "    assert sum([len(x) for x in seq_chunks])-pad_length_last==seq_probas.shape[0]\n",
    "    if strand_info[seq_name]=='-':\n",
    "        seq_probas = seq_probas[::-1,[3,2,1,0]] #reverse complement probabilities\n",
    "    all_preds.append((seq_name,seq_probas))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cea19938-5603-4c33-8628-506e926ddb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + f'motif_predictions/split_75_25/ntrans/NT-MS-v2-500M_{fold}.pickle', 'wb') as f:\n",
    "    pickle.dump(all_preds,f)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323f3d1-1f2f-469d-92cd-acc47d8e5764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
