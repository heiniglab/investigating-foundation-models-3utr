{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aaef155c-efca-43c0-8f0f-e18f33571886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "283206cf-5075-40c0-8b39-62b4cf4c14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 10\n",
    "MAX_TOK_LEN = 1024\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d4b3f4e-5e95-4a93-9271-f77037fe0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7247644-bed7-424d-a4e8-146bcba7d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/MLM/nucleotide-transform/nucleotide-transformer-v2-500m-multi-species'\n",
    "model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/MLM/nucleotide-transform/nucleotide-transformer-v2-250m-multi-species'\n",
    "\n",
    "model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/ntrans-v2-250m-3utr/checkpoints/epoch_31/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec8c7dbf-20bd-4fe2-b593-7f8cf68a3fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse sequences on negative strand before inference: False\n"
     ]
    }
   ],
   "source": [
    "if not '3utr' in model_dir:\n",
    "    #dna model: model trained on sequences that weren't reverse complemented for genes on negative strand\n",
    "    #all NT models from InstaDeep\n",
    "    #the sequences transcribed from the negative strand will be reverse complemented before inference\n",
    "    #after inference, the probabilities will be reverse complemented again to match the original sequence\n",
    "    reverse_seq_neg_strand = True\n",
    "else:\n",
    "    reverse_seq_neg_strand = False\n",
    "\n",
    "print(f'Reverse sequences on negative strand before inference: {reverse_seq_neg_strand}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b8df946-4917-441b-ad30-e37c261ce89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir,trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_dir,trust_remote_code=True).to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62901dc9-228d-489f-a423-756e737d3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(seq_tokens):\n",
    "    '''\n",
    "    Chunk the given token sequence into chunks of MAX_TOK_LEN\n",
    "    The input sequences shouldn't contain special tokens\n",
    "    The last chunk is padded with the previous chunk if it's shorter than MAX_TOK_LEN\n",
    "    '''\n",
    "    if tokenizer.eos_token_id:\n",
    "        #in the original InstaDeep models, the cls token wasn't present\n",
    "        chunk_len = MAX_TOK_LEN-2 #2 special tokens to be added \n",
    "    else:\n",
    "        chunk_len = MAX_TOK_LEN-1 #only cls token\n",
    "    chunks = [seq_tokens[start:start+chunk_len] for start in range(0,len(seq_tokens),chunk_len)]\n",
    "    assert [x for y in chunks for x in y]==seq_tokens\n",
    "    if len(chunks)>1:\n",
    "        left_shift = min(chunk_len-len(chunks[-1]), len(chunks[-2])) #overlap length for the last chunk and the previous one\n",
    "        if left_shift>0:\n",
    "            pad_seq = chunks[-2][-left_shift:]\n",
    "            chunks[-1] = pad_seq + chunks[-1]\n",
    "    else:\n",
    "        left_shift = 0\n",
    "    if tokenizer.eos_token_id:\n",
    "        chunks = [[tokenizer.cls_token_id, *chunk, tokenizer.eos_token_id] for chunk in chunks]\n",
    "        assert [x for y in chunks[:-1] for x in y[1:-1]]+[x for x in  chunks[-1][1+left_shift:-1]]==seq_tokens\n",
    "    else:\n",
    "        chunks = [[tokenizer.cls_token_id, *chunk] for chunk in chunks]\n",
    "        assert [x for y in chunks[:-1] for x in y[1:]]+[x for x in  chunks[-1][1+left_shift:]]==seq_tokens\n",
    "    #left_shift only makes sense for the last chunk, for the other chunks it's 0\n",
    "    res = [(chunk,0) if chunk_idx!=len(chunks)-1 else (chunk,left_shift) for chunk_idx, chunk in enumerate(chunks)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc320d73-2e97-4407-91bb-f02f1f223977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_sequence(seq_tokens, left_shift=0):\n",
    "    '''\n",
    "    Consecutively mask tokens in the sequence and yield each masked position\n",
    "    Don't mask special tokens and the first left_shift sequence tokens\n",
    "    '''    \n",
    "    for mask_pos in range(1+left_shift,len(seq_tokens)):\n",
    "        if seq_tokens[mask_pos] in (tokenizer.eos_token_id,tokenizer.pad_token_id):\n",
    "            break\n",
    "        masked_seq = seq_tokens.clone()\n",
    "        masked_seq[mask_pos] = tokenizer.mask_token_id\n",
    "        yield mask_pos, masked_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c1097eb-3236-404d-b076-fd948f3170b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, seq_df):\n",
    "        \n",
    "        self.seq_df = seq_df\n",
    "        self.start = 0\n",
    "        self.end = len(self.seq_df)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        for seq_idx in range(self.start, self.end):\n",
    "            \n",
    "            seq_info = self.seq_df.iloc[seq_idx]\n",
    "            chunk, left_shift = seq_info.seq\n",
    "            \n",
    "            gt_tokens = torch.LongTensor(chunk)\n",
    "            \n",
    "            for masked_pos, masked_tokens in mask_sequence(gt_tokens, left_shift):\n",
    "                #consecutively mask each token in the sequence\n",
    "                assert masked_tokens[masked_pos] == tokenizer.mask_token_id\n",
    "                yield seq_info.seq_name, gt_tokens, masked_pos, masked_tokens\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "     worker_info = torch.utils.data.get_worker_info()\n",
    "     dataset = worker_info.dataset  # the dataset copy in this worker process\n",
    "     overall_start = dataset.start\n",
    "     overall_end = dataset.end\n",
    "     # configure the dataset to only process the split workload\n",
    "     per_worker = int(np.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
    "     worker_id = worker_info.id\n",
    "     dataset.start = overall_start + worker_id * per_worker\n",
    "     dataset.end = min(dataset.start + per_worker, overall_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a77604-3bc3-4383-9d1b-767c8cbbfe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_batch(masked_tokens_batch):\n",
    "\n",
    "    targets_masked = masked_tokens_batch.clone()\n",
    "    targets_masked[targets_masked!=tokenizer.mask_token_id] = -100\n",
    "    attention_mask = masked_tokens_batch!= tokenizer.pad_token_id   \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        torch_outs = model(\n",
    "        masked_tokens_batch.to(device),\n",
    "        labels = targets_masked.to(device),\n",
    "        attention_mask=attention_mask.to(device),\n",
    "        encoder_attention_mask=attention_mask.to(device),\n",
    "        output_hidden_states=False)\n",
    "    \n",
    "    logits = torch_outs[\"logits\"] #max_tokenized_length x (max_tokenized_length+1) x N_tokens\n",
    "    \n",
    "    probas_batch = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "    \n",
    "    loss = torch_outs[\"loss\"].item()\n",
    "    \n",
    "    return probas_batch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cd367b8-dbec-40ef-82a2-c1ac6484734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_complement(seq):\n",
    "    '''\n",
    "    Take sequence reverse complement\n",
    "    '''\n",
    "    compl_dict = {'A':'T', 'C':'G', 'G':'C', 'T':'A'}\n",
    "    compl_seq = ''.join([compl_dict.get(x,x) for x in seq])\n",
    "    rev_seq = compl_seq[::-1]\n",
    "    return rev_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "963b87d9-b877-4898-90bb-de51e5266dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokendict_list = [{\"A\": [], \"G\": [], \"T\": [],\"C\": []} for x in range(6)]\n",
    "\n",
    "for tpl in itertools.product(\"ACGT\",repeat=6):\n",
    "    encoding = tokenizer.encode(\"\".join(tpl))\n",
    "    for idx, nuc in enumerate(tpl):\n",
    "        tokendict_list[idx][nuc].append(encoding[1]) #token indices for idx position in 6-mer and letter nuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58b41166-2b2e-40d3-8ca4-6ab8d94cd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/fasta/Homo_sapiens_rna.fa'\n",
    "\n",
    "seq_df = defaultdict(str)\n",
    "\n",
    "with open(fasta, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            seq_name = line[1:].rstrip()\n",
    "        else:\n",
    "            seq_df[seq_name] += line.rstrip().upper()\n",
    "            \n",
    "seq_df = pd.DataFrame(list(seq_df.items()), columns=['seq_name','seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19dc2de3-8648-4817-be72-911f1f378c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e93f9f3b-96b5-4b28-a154-55e1421e8d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1078 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 1814 sequences\n"
     ]
    }
   ],
   "source": [
    "folds = np.arange(N_FOLDS).repeat(len(seq_df)//N_FOLDS+1)[:len(seq_df)] #split into folds \n",
    "\n",
    "seq_df = seq_df.loc[folds==fold] #get required fold\n",
    "\n",
    "print(f'Fold {fold}: {len(seq_df)} sequences')\n",
    "\n",
    "#reverse complement on the negative strand if reverse_seq_neg_strand=True\n",
    "strand_info = pd.read_csv(data_dir + 'UTR_coords/GRCh38_3_prime_UTR_clean-sorted.bed', sep='\\t', header = None, names=['seq_name','strand'], usecols=[3,5]).set_index('seq_name').squeeze()\n",
    "seq_df.seq = seq_df.apply(lambda x: reverse_complement(x.seq) if strand_info.loc[x.seq_name]=='-' \n",
    "                          and reverse_seq_neg_strand else x.seq, axis=1) #undo reverse complement\n",
    "\n",
    "original_seqs = seq_df.set_index('seq_name').seq #sequences before tokenization\n",
    "\n",
    "seq_df['seq'] = seq_df.seq.apply(lambda x: tokenizer(x,add_special_tokens=False)['input_ids']) #tokenize sequences\n",
    "seq_df['seq'] = seq_df.seq.apply(lambda x:get_chunks(x)) #chunk sequences\n",
    "\n",
    "seq_df = seq_df.explode('seq') #each chunk gets a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a3221d6-03b2-4db5-9582-43b4bf8e20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_tokens = seq_df.seq.apply(lambda x:len(x[0])-2-x[1]).sum() #minus special tokens minus overlap for last chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba0fbd82-ddeb-41a5-9c39-e7f65fdb747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    '''\n",
    "    Collate tokenized sequences based on the maximal sequence length in the batch\n",
    "    '''\n",
    "    seq_names_batch, gt_tokens_batch, masked_pos_batch, masked_tokens_batch = zip(*batch)\n",
    "    masked_tokens_batch = pad_sequence(masked_tokens_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    gt_tokens_batch = pad_sequence(gt_tokens_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return seq_names_batch, gt_tokens_batch, masked_pos_batch, masked_tokens_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "460ba37a-b224-450c-8799-83ec4b6f8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(SeqDataset(seq_df), batch_size=BATCH_SIZE, \n",
    "                        shuffle=False, collate_fn=collate_fn, \n",
    "                        num_workers=1, worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e91a5089-0996-4065-8b10-7888122f878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1814 [00:13<6:42:47, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000641515.2_utr3_2_0_chr1_70009_f processed (1/1814), loss: 22.7, acc:0.375\n",
      "Sequence ENST00000616016.5_utr3_13_0_chr1_944154_f processed (2/1814), loss: 22.7, acc:0.379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1814 [00:14<1:56:07,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000327044.7_utr3_18_0_chr1_944203_r processed (3/1814), loss: 22.3, acc:0.378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1814 [00:16<1:40:33,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000338591.8_utr3_11_0_chr1_965192_f processed (4/1814), loss: 22.3, acc:0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1814 [00:19<1:32:02,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000379410.8_utr3_15_0_chr1_974576_f processed (5/1814), loss: 22.3, acc:0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1814 [00:21<1:20:56,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000433179.4_utr3_3_0_chr1_975198_r processed (6/1814), loss: 22.4, acc:0.396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1814 [00:23<1:14:48,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000304952.11_utr3_3_0_chr1_998964_r processed (7/1814), loss: 22.3, acc:0.397\n",
      "Sequence ENST00000649529.1_utr3_1_0_chr1_1014479_f processed (8/1814), loss: 22.3, acc:0.398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/1814 [00:25<55:07,  1.83s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000379370.7_utr3_35_0_chr1_1054982_f processed (9/1814), loss: 22.3, acc:0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1814 [00:27<56:45,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000453464.3_utr3_1_0_chr1_1070967_r processed (10/1814), loss: 22.1, acc:0.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1814 [00:31<1:13:31,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000421241.7_utr3_9_0_chr1_1081823_r processed (11/1814), loss: 22.1, acc:0.41\n",
      "Sequence ENST00000379289.6_utr3_15_0_chr1_1197848_f processed (12/1814), loss: 22.1, acc:0.411\n",
      "Sequence ENST00000379268.7_utr3_4_0_chr1_1203508_r processed (13/1814), loss: 22.1, acc:0.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1814 [00:34<52:25,  1.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000379236.4_utr3_6_0_chr1_1211340_r processed (14/1814), loss: 21.8, acc:0.414\n",
      "Sequence ENST00000360001.12_utr3_6_0_chr1_1216931_r processed (15/1814), loss: 21.8, acc:0.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/1814 [00:45<1:26:35,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000379198.5_utr3_0_0_chr1_1233269_f processed (16/1814), loss: 21.6, acc:0.403\n",
      "Sequence ENST00000330388.2_utr3_7_0_chr1_1242453_r processed (17/1814), loss: 21.6, acc:0.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/1814 [00:49<1:21:55,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000349431.11_utr3_6_0_chr1_1253912_r processed (18/1814), loss: 21.5, acc:0.4\n",
      "Sequence ENST00000379116.10_utr3_17_0_chr1_1291611_f processed (19/1814), loss: 21.5, acc:0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/1814 [00:54<1:17:04,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000354700.10_utr3_23_0_chr1_1292391_r processed (20/1814), loss: 21.6, acc:0.4\n",
      "Sequence ENST00000379031.10_utr3_7_0_chr1_1311380_f processed (21/1814), loss: 21.6, acc:0.399\n",
      "Sequence ENST00000435064.6_utr3_16_0_chr1_1311600_r processed (22/1814), loss: 21.6, acc:0.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 23/1814 [00:58<1:03:25,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000343938.9_utr3_2_0_chr1_1327764_f processed (23/1814), loss: 21.8, acc:0.398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 24/1814 [01:00<1:01:15,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000339381.6_utr3_5_0_chr1_1334465_f processed (24/1814), loss: 21.8, acc:0.398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 25/1814 [01:02<59:39,  2.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378888.10_utr3_14_0_chr1_1335278_r processed (25/1814), loss: 21.8, acc:0.398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 26/1814 [01:04<58:13,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000309212.11_utr3_9_0_chr1_1352691_r processed (26/1814), loss: 21.9, acc:0.397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 27/1814 [01:06<1:04:25,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000338338.10_utr3_3_0_chr1_1373736_r processed (27/1814), loss: 21.9, acc:0.397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 28/1814 [01:12<1:32:14,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000400809.8_utr3_10_0_chr1_1385711_r processed (28/1814), loss: 21.9, acc:0.392\n",
      "Sequence ENST00000344843.12_utr3_3_0_chr1_1401909_r processed (29/1814), loss: 21.9, acc:0.392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 30/1814 [01:17<1:24:01,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000537107.6_utr3_3_0_chr1_1418420_r processed (30/1814), loss: 21.8, acc:0.391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 31/1814 [01:33<2:53:56,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378821.4_utr3_1_0_chr1_1427788_f processed (31/1814), loss: 21.7, acc:0.411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 32/1814 [01:58<5:15:34, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000476993.2_utr3_2_0_chr1_1439788_f processed (32/1814), loss: 21.7, acc:0.406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 33/1814 [02:06<4:47:44,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378785.7_utr3_11_0_chr1_1468531_f processed (33/1814), loss: 21.7, acc:0.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 34/1814 [02:17<5:01:55, 10.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000673477.1_utr3_15_0_chr1_1495818_f processed (34/1814), loss: 21.7, acc:0.417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 36/1814 [02:18<2:43:52,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378756.8_utr3_15_0_chr1_1534073_f processed (35/1814), loss: 21.6, acc:0.417\n",
      "Sequence ENST00000378733.9_utr3_3_0_chr1_1534778_r processed (36/1814), loss: 21.6, acc:0.417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 37/1814 [02:21<2:17:28,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000291386.4_utr3_4_0_chr1_1541673_r processed (37/1814), loss: 21.6, acc:0.417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 38/1814 [02:26<2:20:23,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000422725.4_utr3_0_0_chr1_1598012_r processed (38/1814), loss: 21.6, acc:0.416\n",
      "Sequence ENST00000355826.10_utr3_19_0_chr1_1630531_f processed (39/1814), loss: 21.6, acc:0.416\n",
      "Sequence ENST00000356026.10_utr3_7_0_chr1_1634626_f processed (40/1814), loss: 21.6, acc:0.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 41/1814 [02:35<1:52:26,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000341832.11_utr3_19_0_chr1_1635225_r processed (41/1814), loss: 21.6, acc:0.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 42/1814 [03:20<6:08:26, 12.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000617444.5_utr3_9_0_chr1_1661478_r processed (42/1814), loss: 21.6, acc:0.414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 43/1814 [03:23<5:05:31, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000404249.8_utr3_19_0_chr1_1702379_r processed (43/1814), loss: 21.6, acc:0.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 44/1814 [03:30<4:37:32,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000341426.9_utr3_11_0_chr1_1751232_r processed (44/1814), loss: 21.6, acc:0.411\n",
      "Sequence ENST00000307786.8_utr3_5_0_chr1_1917194_f processed (45/1814), loss: 21.6, acc:0.411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 46/1814 [03:31<2:48:05,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000310991.8_utr3_4_0_chr1_1917591_r processed (46/1814), loss: 21.6, acc:0.411\n",
      "Sequence ENST00000682832.2_utr3_38_0_chr1_1921957_r processed (47/1814), loss: 21.6, acc:0.411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 48/1814 [03:38<2:23:56,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378585.7_utr3_8_0_chr1_2030283_f processed (48/1814), loss: 21.6, acc:0.411\n",
      "Sequence ENST00000378567.8_utr3_17_0_chr1_2185010_f processed (49/1814), loss: 21.6, acc:0.41\n",
      "Sequence ENST00000378546.9_utr3_3_0_chr1_2189548_r processed (50/1814), loss: 21.6, acc:0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 51/1814 [04:13<3:52:17,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378536.5_utr3_6_0_chr1_2306766_f processed (51/1814), loss: 21.5, acc:0.409\n",
      "Sequence ENST00000378531.8_utr3_13_0_chr1_2321253_r processed (52/1814), loss: 21.5, acc:0.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 53/1814 [04:27<3:42:55,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000605895.6_utr3_6_0_chr1_2403125_f processed (53/1814), loss: 21.4, acc:0.406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 54/1814 [04:34<3:39:28,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000447513.7_utr3_5_0_chr1_2403974_r processed (54/1814), loss: 21.4, acc:0.404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 55/1814 [04:35<3:03:13,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378486.8_utr3_21_0_chr1_2505214_f processed (55/1814), loss: 21.4, acc:0.404\n",
      "Sequence ENST00000378466.9_utr3_18_0_chr1_2508537_r processed (56/1814), loss: 21.4, acc:0.404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 57/1814 [04:36<2:04:07,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378453.4_utr3_2_0_chr1_2528745_r processed (57/1814), loss: 21.4, acc:0.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 58/1814 [04:40<2:01:40,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000355716.5_utr3_7_0_chr1_2563274_f processed (58/1814), loss: 21.4, acc:0.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 60/1814 [04:58<2:37:50,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000419916.8_utr3_6_0_chr1_2589428_f processed (59/1814), loss: 21.5, acc:0.401\n",
      "Sequence ENST00000378412.8_utr3_23_0_chr1_2590639_r processed (60/1814), loss: 21.5, acc:0.401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 61/1814 [05:54<9:06:25, 18.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000401095.9_utr3_8_0_chr1_2636986_r processed (61/1814), loss: 21.6, acc:0.398\n",
      "Sequence ENST00000378404.4_utr3_0_0_chr1_3022821_f processed (62/1814), loss: 21.6, acc:0.397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 63/1814 [06:57<11:43:21, 24.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000270722.10_utr3_16_0_chr1_3433812_f processed (63/1814), loss: 21.5, acc:0.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 64/1814 [07:02<9:33:19, 19.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378378.9_utr3_14_0_chr1_3480588_f processed (64/1814), loss: 21.5, acc:0.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 65/1814 [07:17<9:01:11, 18.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000356575.9_utr3_36_0_chr1_3487951_r processed (65/1814), loss: 21.6, acc:0.394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 66/1814 [07:23<7:21:58, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378344.7_utr3_4_0_chr1_3628604_f processed (66/1814), loss: 21.6, acc:0.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 67/1814 [07:29<6:10:22, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000270708.12_utr3_11_0_chr1_3630770_r processed (67/1814), loss: 21.6, acc:0.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 68/1814 [07:54<7:52:38, 16.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378295.9_utr3_13_0_chr1_3733080_f processed (68/1814), loss: 21.6, acc:0.393\n",
      "Sequence ENST00000294600.7_utr3_11_0_chr1_3771524_f processed (69/1814), loss: 21.6, acc:0.394\n",
      "Sequence ENST00000642557.4_utr3_3_0_chr1_3775922_f processed (70/1814), loss: 21.6, acc:0.394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 71/1814 [08:11<5:02:17, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378251.3_utr3_6_0_chr1_3778559_r processed (71/1814), loss: 21.6, acc:0.394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 72/1814 [08:45<7:20:36, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence ENST00000378230.8_utr3_21_0_chr1_3812086_r processed (72/1814), loss: 21.5, acc:0.399\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(original_seqs))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_names_batch, gt_tokens_batch, masked_pos_batch, masked_tokens_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 15\u001b[0m     probas_batch, loss_batch \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_tokens_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#probas_batch, loss_batch = np.zeros((len(seq_names_batch),1024,4108)), 0\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     all_losses\u001b[38;5;241m.\u001b[39mappend(loss_batch)\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mpredict_on_batch\u001b[0;34m(masked_tokens_batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m     torch_outs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m      9\u001b[0m     masked_tokens_batch\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     10\u001b[0m     labels \u001b[38;5;241m=\u001b[39m targets_masked\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     11\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     12\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     13\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch_outs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m#max_tokenized_length x (max_tokenized_length+1) x N_tokens\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m probas_batch \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch_outs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probas_batch, loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3} #for accuracy\n",
    "\n",
    "all_probas = defaultdict(list) #probas for all masked tokens \n",
    "verif_seqs = defaultdict(str) #reconstruct sequences from mask tokens and make sure that they match the original sequences\n",
    "\n",
    "all_losses, is_correct = [], [] #to compute loss and accuracy\n",
    "prev_seq_name = None #name of the previous sequence\n",
    "\n",
    "pbar = tqdm(total=len(original_seqs))\n",
    "\n",
    "for seq_names_batch, gt_tokens_batch, masked_pos_batch, masked_tokens_batch in dataloader:\n",
    "\n",
    "    probas_batch, loss_batch = predict_on_batch(masked_tokens_batch)\n",
    "    #probas_batch, loss_batch = np.zeros((len(seq_names_batch),1024,4108)), 0 #placeholder for testing\n",
    "    \n",
    "    all_losses.append(loss_batch)\n",
    "    \n",
    "    for seq_name, gt_tokens, masked_pos, seq_probas in zip(seq_names_batch, gt_tokens_batch, masked_pos_batch, probas_batch):\n",
    "        gt_token = tokenizer.id_to_token(gt_tokens[masked_pos].item()) #ground truth masked token\n",
    "        for idx in range(len(gt_token)):\n",
    "            #loop over all positions of the masked token\n",
    "            position_probas = [] #probabilities for all bases at given position\n",
    "            for nuc in 'ACGT':\n",
    "                position_probas.append(seq_probas[masked_pos][tokendict_list[idx][nuc]].sum()) #sum over all tokens that have given letter at given position\n",
    "            all_probas[seq_name].append(position_probas)\n",
    "        if seq_name!=prev_seq_name:\n",
    "            #processing of prev_seq_name is completed\n",
    "            if len(verif_seqs[prev_seq_name])>0:\n",
    "                is_correct.extend([nuc_dict.get(base,4)==gt_idx for base, gt_idx in zip(verif_seqs[prev_seq_name],np.argmax(all_probas[prev_seq_name],axis=1))])\n",
    "                print(f'Sequence {prev_seq_name} processed ({len(verif_seqs)-1}/{len(original_seqs)}), loss: {np.mean(all_losses):.3}, acc:{np.mean(is_correct):.3}')\n",
    "                assert verif_seqs[prev_seq_name]==original_seqs.loc[prev_seq_name] #compare reconstruction from the masked token with the original sequence\n",
    "                pbar.update(1)\n",
    "            prev_seq_name = seq_name\n",
    "        verif_seqs[seq_name] += gt_token      \n",
    "\n",
    "assert verif_seqs[seq_name]==original_seqs.loc[seq_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d0aeb-b34e-43b7-84b6-4f2af2c8f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_names = list(all_probas.keys())\n",
    "probs = [np.array(x) for x in all_probas.values()]\n",
    "seqs = original_seqs.loc[seq_names].values.tolist()\n",
    "\n",
    "if reverse_seq_neg_strand:\n",
    "    probs = [x[::-1,[3,2,1,0]] if strand_info.loc[seq_name]=='-' else x for x, seq_name in zip(probs,seq_names)]\n",
    "    seqs = [reverse_complement(x) if strand_info.loc[seq_name]=='-' else x for x, seq_name in zip(seqs,seq_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea19938-5603-4c33-8628-506e926ddb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(data_dir + f'motif_predictions/split_75_25/ntrans/NT-MS-v2-500M_{fold}.pickle', 'wb') as f:\n",
    "#    pickle.dump({'seq_names':seq_names,'seqs':seqs, 'probs':probs, 'fasta':fasta},f)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323f3d1-1f2f-469d-92cd-acc47d8e5764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
