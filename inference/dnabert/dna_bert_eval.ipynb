{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc9794c-c682-4f51-8295-36477b6c6c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig, BertForMaskedLM\n",
    "\n",
    "import torch \n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "from collections.abc import Mapping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971b3b99-0a57-451f-9f0a-3dd7793ceeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/MLM/'\n",
    "\n",
    "model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/dnabert/6-new-12w-0/'\n",
    "model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/dnabert-3utr/checkpoints/epoch_30/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a55430e-7bee-4b2c-a7c5-1daf6b23de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5293e19d-3b08-48f3-9b57-d53c122e74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/dnabert-3utr/checkpoints/epoch_30/'\n",
    "#model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/dnabert/6-new-12w-0/'\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "#tokenizer.encode('GGGGGG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a93531-031b-4f59-a586-61d73eda558e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not '3utr' in model_dir:\n",
    "    #dna model: model trained on sequences that weren't reverse complemented for genes on negative strand\n",
    "    #all default DNABERT models\n",
    "    dna_model = True\n",
    "else:\n",
    "    dna_model = False\n",
    "\n",
    "dna_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a16e7-d43d-4817-83e9-09842ada9262",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32485ee-d112-4d0b-8d02-34049891cdaf",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa321a1c-e4a8-4efb-8bd0-65588c985d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "\n",
    "def chunkstring(string, length):\n",
    "    # chunks a string into segments of length\n",
    "    return (string[0+i:length+i] for i in range(0, len(string), length))\n",
    "\n",
    "def kmers(seq, k=6):\n",
    "    # splits a sequence into non-overlappnig k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq), k) if i + k <= len(seq)]\n",
    "\n",
    "def kmers_stride1(seq, k=6):\n",
    "    # splits a sequence into overlapping k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq)-k+1)]   \n",
    "\n",
    "def tok_func(x): return tokenizer(\" \".join(kmers_stride1(x[\"seq_chunked\"])))\n",
    "\n",
    "def one_hot_encode(gts, dim=5):\n",
    "    result = []\n",
    "    for nt in gts:\n",
    "        vec = np.zeros(dim)\n",
    "        vec[nuc_dict[nt]] = 1\n",
    "        result.append(vec)\n",
    "    return np.stack(result, axis=0)\n",
    "\n",
    "def class_label_gts(gts):\n",
    "    return np.array([nuc_dict[x] for x in gts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63eb49f-0ab2-4f0c-8557-f476c4485ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers import  DataCollatorForLanguageModeling\n",
    "#data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability = 0.15)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of = None):\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple, np.ndarray)):\n",
    "        examples = [torch.tensor(e, dtype=torch.long) for e in examples]\n",
    "\n",
    "    length_of_first = examples[0].size(0)\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "\n",
    "    are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return torch.stack(examples, dim=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(x.size(0) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
    "    for i, example in enumerate(examples):\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            result[i, : example.shape[0]] = example\n",
    "        else:\n",
    "            result[i, -example.shape[0] :] = example\n",
    "    return result\n",
    "\n",
    "class DataCollatorForLanguageModelingSpan():\n",
    "    def __init__(self, tokenizer, mlm, mlm_probability, span_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm = mlm\n",
    "        self.span_length =span_length\n",
    "        self.mlm_probability= mlm_probability\n",
    "        self.pad_to_multiple_of = span_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n",
    "                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "            )\n",
    "        else:\n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def torch_mask_tokens(self, inputs, special_tokens_mask):\n",
    "        import torch\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability*0.2)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool().numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * self.span_length, mode = 'same' ),axis = 1, arr = masked_indices).astype(bool) \n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        m_save = masked_indices.clone()\n",
    "        \n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability*0.8) \n",
    "        probability_matrix.masked_fill_(masked_indices, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool().numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * self.span_length, mode = 'same' ),axis = 1, arr = masked_indices).astype(bool) \n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        m_final = masked_indices + m_save \n",
    "        labels[~m_final] = -100  # We only compute loss on masked tokens\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        #indices_replaced = torch.bernoulli(torch.full(labels.shape, 1.0)).bool()\n",
    "        #print (indices_replaced)\n",
    "        inputs[masked_indices] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        #print (masked_indices)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        #indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        #random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        #inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b1cbc-11ab-4355-acb8-41a92d826060",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d3c3a7-f9d8-4d48-aa48-19a1fca8c1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_batch(tokenized_data, dataset, seq_idx):\n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    label = dataset.iloc[seq_idx]['seq_chunked']\n",
    "    label_len = len(label)\n",
    "    if label_len < 6:\n",
    "        return torch.zeros(label_len,label_len,5), None\n",
    "    else:\n",
    "        diag_matrix = torch.eye(tokenized_data['input_ids'].shape[1]).numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * 6, mode = 'same' ),axis = 1, arr = diag_matrix).astype(bool)\n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        masked_indices = masked_indices[3:label_len-5-2]\n",
    "        res = tokenized_data['input_ids'].expand(masked_indices.shape[0],-1).clone()\n",
    "        targets_masked = res.clone().to(device)\n",
    "        res[masked_indices] = 4\n",
    "        targets_masked[res!=4] = -100\n",
    "        #print (res[0], res.shape)\n",
    "        res = res.to(device)\n",
    "        with torch.no_grad():\n",
    "            model_outs = model(res,labels=targets_masked)\n",
    "            fin_calculation = torch.softmax(model_outs['logits'], dim=2).detach().cpu()   \n",
    "        return fin_calculation, model_outs['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9fe45-2c83-445f-b990-1f0dc8f7efbb",
   "metadata": {},
   "source": [
    "## Translating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a93927f-c4c5-4f4f-8452-5cbfdb022e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_prbs_from_pred(prediction, pred_pos, token_pos, label_pos, label):   \n",
    "    # pred_pos = \"kmer\" position in tokenized sequence (incl. special tokens)\n",
    "    # token_pos = position of nucleotide in kmer\n",
    "    # label_pos = position of actual nucleotide in sequence\n",
    "    model_pred = prediction\n",
    "    prbs = [torch.sum(model_pred[pred_pos,tokendict_list[token_pos][nuc]]) for nuc in [\"A\",\"C\",\"G\",\"T\"]]\n",
    "    gt = label[label_pos] # 6-CLS, zerobased\n",
    "    res = torch.tensor(prbs+[0.0])\n",
    "    return res, gt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca263894-392a-44f5-bb47-f0ec30eebbe9",
   "metadata": {},
   "source": [
    "# Prepare inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a469f-dc86-465a-81bc-3a7923631f3c",
   "metadata": {},
   "source": [
    "## Prepare dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e85d91d9-c486-47b0-a58d-4d8985ddf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_complement(seq):\n",
    "    '''\n",
    "    Take sequence reverse complement\n",
    "    '''\n",
    "    compl_dict = {'A':'T', 'C':'G', 'G':'C', 'T':'A'}\n",
    "    compl_seq = ''.join([compl_dict.get(x,x) for x in seq])\n",
    "    rev_seq = compl_seq[::-1]\n",
    "    return rev_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b634469c-a9ae-4bb2-8527-44d08387c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_folds = 10\n",
    "fold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d93af7b3-e99d-456a-89b9-3c505850a2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fasta = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/fasta/Homo_sapiens_rna.fa'\n",
    "\n",
    "dataset = defaultdict(str)\n",
    "\n",
    "with open(fasta, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            seq_name = line[1:].rstrip()\n",
    "        else:\n",
    "            dataset[seq_name] += line.rstrip().upper()\n",
    "            \n",
    "dataset = pd.DataFrame(list(dataset.items()), columns=['seq_name','seq'])\n",
    "\n",
    "folds = np.arange(N_folds).repeat(len(dataset)//N_folds+1)[:len(dataset)] \n",
    "\n",
    "dataset = dataset.loc[folds==fold]\n",
    "\n",
    "strand_info = pd.read_csv(data_dir + 'UTR_coords/GRCh38_3_prime_UTR_clean-sorted.bed', sep='\\t', header = None, names=['seq_name','strand'], usecols=[3,5]).set_index('seq_name').squeeze()\n",
    "\n",
    "dataset['original_seq'] = dataset['seq'] \n",
    "\n",
    "dataset['seq'] = dataset.apply(lambda x: reverse_complement(x.seq) if strand_info.loc[x.seq_name]=='-' and dna_model else x.seq, axis=1) #undo reverse complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "599901ca-0b3a-44d8-9d9f-f380b702ed4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['seq_chunked'] = dataset['seq'].apply(lambda x : list(chunkstring(x, 510))) #chunk string in segments of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b57022c-c120-43e8-a76e-3eba91f4e209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#output_dir = '/s/project/mll/sergey/effect_prediction/MLM/motif_predictions/split_75_25/dnabert/default/'\n",
    "#\n",
    "#dataset_len = 500\n",
    "#\n",
    "#for dataset_start in range(0,len(dataset),dataset_len):\n",
    "#    df = dataset.iloc[dataset_start:dataset_start+dataset_len]\n",
    "#    df[['seq_name','seq']].to_csv(output_dir + f'/seq_{dataset_start}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2319883a-446c-4d6b-ac3e-ac87d87a5de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.explode('seq_chunked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "230e0d37-9326-4db7-8a16-76afc6247be9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a8401de7264db0a7e5d6fa24574753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/8359 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(dataset[['seq_chunked']])\n",
    "\n",
    "tok_ds = ds.map(tok_func, batched=False,  num_proc=2)\n",
    "\n",
    "rem_tok_ds = tok_ds.remove_columns('seq_chunked')\n",
    "\n",
    "data_collator = DataCollatorForLanguageModelingSpan(tokenizer, mlm=False, mlm_probability = 0.025, span_length =6)\n",
    "data_loader = torch.utils.data.DataLoader(rem_tok_ds, batch_size=1, collate_fn=data_collator, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed132ad6-0aad-4cab-8dde-919719c02abc",
   "metadata": {},
   "source": [
    "## Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04f98016-880a-43c7-993c-35fe8fd5b69b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44910007-d73d-4499-83b3-2fec77d686be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "computed = []\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd982373-8c17-46c0-abed-7e1c72df1eed",
   "metadata": {},
   "source": [
    "## Prepare tokendict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a615b9f0-185f-4de2-a6fe-910cc6c992be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokendict_list = [{\"A\": [], \"G\": [], \"T\": [],\"C\": []} for x in range(6)]\n",
    "\n",
    "for tpl in itertools.product(\"ACGT\",repeat=6):\n",
    "    encoding = tokenizer.encode(\"\".join(tpl))\n",
    "    for idx, nuc in enumerate(tpl):\n",
    "        tokendict_list[idx][nuc].append(encoding[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a7a66-b2bf-4525-b2e5-ecff6ae664f1",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f00660-95e0-4d15-b7d7-54c349eab162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 6\n",
    "predicted_prbs,gts,is_correct,loss = [],[],[],[]\n",
    "#print (dataset.iloc[0]['seq_chunked'])\n",
    "\n",
    "for no_of_index, tokenized_data in tqdm.tqdm(enumerate(data_loader)):\n",
    "    #if no_of_index < 1340:\n",
    "    #    continue\n",
    "    label = dataset.iloc[no_of_index]['seq_chunked']\n",
    "    label_len = len(label)\n",
    "    #print(no_of_index, label_len)\n",
    "    \n",
    "    # Edge case: for a sequence less then 11 nt\n",
    "    # we cannot even feed 6 mask tokens\n",
    "    # so we might as well predict random\n",
    "    if label_len < 11: \n",
    "        #print (no_of_index)\n",
    "        for i in range(label_len):\n",
    "            predicted_prbs.append(torch.tensor([0.25,0.25,0.25,0.25,0.0]))\n",
    "            gts.append(label[i])\n",
    "            is_correct.append(res.argmax().item() == nuc_dict.get(gt,4))\n",
    "        continue\n",
    "\n",
    "        \n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    tokenized_data['labels'][tokenized_data['labels']==-100] = 0\n",
    "    inputs = model_input_unaltered.clone()\n",
    "    \n",
    "\n",
    "    # First 5 nucleotides we infer from the first 6-mer\n",
    "    inputs[:, 1:7] = 4 # we mask the first 6 6-mers\n",
    "    inputs = inputs.to(device) \n",
    "\n",
    "    targets_masked = model_input_unaltered.clone().to(device)\n",
    "    targets_masked[inputs!=4] = -100\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_outs = model(inputs,labels=targets_masked)\n",
    "        \n",
    "    model_pred = torch.softmax(model_outs['logits'], dim=2)\n",
    "    loss.append(model_outs['loss'].item())\n",
    "    \n",
    "    for i in range(5):\n",
    "        res,gt = extract_prbs_from_pred(prediction=model_pred[0],\n",
    "                                        pred_pos=1, # first 6-mer (after CLS)\n",
    "                                        token_pos=i, # we go thorugh first 6-mer\n",
    "                                        label_pos=i,\n",
    "                                        label=label)\n",
    "        predicted_prbs.append(res)\n",
    "        gts.append(gt)\n",
    "        is_correct.append(res.argmax() == nuc_dict.get(gt,4))\n",
    "\n",
    "    \n",
    "    \n",
    "    # we do a batched predict to process the rest of the sequence\n",
    "    predictions,seq_loss = predict_on_batch(tokenized_data, dataset, no_of_index)\n",
    "    if seq_loss is not None:\n",
    "        loss.append(seq_loss.item())\n",
    "    \n",
    "    # For the 6th nt up to the last 5 \n",
    "    # we extract probabilities similar to how the model was trained\n",
    "    # hiding the 4th nt of the 3rd masked 6-mer of a span of 6 masked 6-mers\n",
    "    # note that CLS makes the tokenized seq one-based\n",
    "    pos = 5 # position in sequence\n",
    "    for pos in range(5, label_len-5):\n",
    "        model_pred = predictions[pos-5]\n",
    "        res,gt = extract_prbs_from_pred(prediction=model_pred,\n",
    "                                        pred_pos=pos-2, # for i-th nt, we look at (i-2)th 6-mer\n",
    "                                        token_pos=3, # look at 4th nt in 6-mer\n",
    "                                        label_pos=pos,\n",
    "                                        label=label)\n",
    "        predicted_prbs.append(res)\n",
    "        gts.append(gt)\n",
    "        is_correct.append(res.argmax() == nuc_dict.get(gt,4))\n",
    "\n",
    "    # Infer the last 5 nt from the last 6-mer\n",
    "    for i in range(5):\n",
    "        model_pred = predictions[pos-5]\n",
    "        res,gt = extract_prbs_from_pred(prediction=model_pred,\n",
    "                                pred_pos=pos+1, # len - 5 + 1 = last 6-mer (1-based)\n",
    "                                token_pos=i+1, # we go through last 5 of last 6-mer\n",
    "                                label_pos=pos+i,\n",
    "                                label=label)\n",
    "        predicted_prbs.append(res)\n",
    "        gts.append(gt)\n",
    "        is_correct.append(res.argmax() == nuc_dict.get(gt,4))\n",
    "\n",
    "    assert(len(gts) == torch.stack(predicted_prbs).shape[0]), \"{} iter, expected len:{} vs actual len:{}\".format(no_of_index,\n",
    "                                                                                   len(gts), \n",
    "                                                                     torch.stack(predicted_prbs).shape[0])\n",
    "    print(f'chunks:{no_of_index+1}, acc:{np.mean(is_correct):.3f}, loss:{np.mean(loss):.3f}')\n",
    "    #XABCDEFGHIJKL -> XABCDE [ABCDEF BCDEFG CDEFGH DEFGHI EFGHIJ FGHIJK] GHIJKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8cfc06-e87f-4144-abc6-35a44254c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prbs = np.array(predicted_prbs)[:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d594e75-1254-4b05-9490-4263dba58f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['seq_name','original_seq']].drop_duplicates()\n",
    "dataset['seq_len'] = dataset.original_seq.apply(len)\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "s = 0\n",
    "\n",
    "for seq_name, original_seq, seq_len in dataset.values.tolist():\n",
    "    seq_probas = predicted_prbs[s:s+seq_len,:]\n",
    "    s += seq_len\n",
    "    if strand_info[seq_name]=='-' and dna_model:\n",
    "        seq_probas = seq_probas[::-1,[3,2,1,0]] #reverse complement probabilities s.t. probas match original_seq\n",
    "    all_preds.append((seq_name,original_seq, seq_probas))\n",
    "\n",
    "with open(output_dir + f\"/predictions_{fold}.pickle\", \"wb\") as f:\n",
    "    seq_names, seqs, probs = zip(*all_preds)\n",
    "    pickle.dump({'seq_names':seq_names, 'seqs':seqs, 'probs':probs, 'fasta':fasta},f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564eb1b9-1249-464a-ad28-4de86b1feb43",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24320f1c-c55c-4a64-942c-aa9f24307cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.max(prbs_arr,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896b80c-0362-460e-9cfc-013689861d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "np.sum(gts == np.array([\"A\",\"C\",\"G\",\"T\"])[np.argmax(prbs_arr,axis=1)])/len(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25411478-cf13-4ecc-bcaf-bc9591af8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nt in [\"A\", \"C\", \"G\", \"T\"]:\n",
    "    nt_arr = np.array([nt]*len(gts))\n",
    "    actual = np.sum(gts == nt_arr)/len(gts)\n",
    "    predicted = np.sum(np.array([\"A\",\"C\",\"G\",\"T\"])[np.argmax(prbs_arr,axis=1)] == nt_arr)/len(gts)\n",
    "    print(\"{}: Actual {}, Predicted {}\".format(nt, actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42eec2b-ec52-4142-86f2-025b4387953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prbs = torch.log(torch.stack(predicted_prbs)[:,:-1])\n",
    "class_labels = torch.tensor(class_label_gts(gts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7e2e8-b962-4f14-82d4-4d512a7b11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.nll_loss(log_prbs, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c2aa84-aa6d-418c-beeb-d64fcbdbe56e",
   "metadata": {},
   "source": [
    "# Make data fit metrics handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa217b54-a115-4c44-bd8b-af28858fd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_path = \"outputs/gpar_bertadn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5821776e-d73e-48fe-9517-a70d8fc86b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get targets\n",
    "targets = torch.tensor(class_label_gts(gts))\n",
    "stacked_prbs = torch.stack(predicted_prbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ac2f0-f18b-42cd-8ef3-a0f145bd32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cross entropy, it's already as probability so just nll\n",
    "ce = torch.nn.functional.nll_loss(stacked_prbs, targets, reduction=\"none\") #cross_entropy(prbs, targets)\n",
    "\n",
    "#print(ce)\n",
    "\n",
    "# save\n",
    "torch.save(stacked_prbs,  out_path+\"masked_logits.pt\") # no logits, so use prbs\n",
    "torch.save(torch.argmax(stacked_prbs, dim=1),  out_path+\"masked_preds.pt\")\n",
    "torch.save(stacked_prbs,  out_path+\"prbs.pt\")\n",
    "torch.save(ce, out_path+\"ce.pt\")\n",
    "\n",
    "# save targets\n",
    "torch.save(targets, out_path+\"masked_targets.pt\")\n",
    "\n",
    "# save rest as placeholders (zeros of same length)\n",
    "torch.save(torch.zeros(len(stacked_prbs)), out_path+\"masked_motifs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6acad-bbbd-47c1-81bd-4fd9a5e5b7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
