{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaef155c-efca-43c0-8f0f-e18f33571886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel, BertForMaskedLM\n",
    "\n",
    "#from DNABERT2.bert_layers import BertModel as DNABERT2\n",
    "from DNABERT.src.transformers.tokenization_dna import DNATokenizer\n",
    "\n",
    "import helpers.misc as misc\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7440f12c-8fe4-435c-842a-ba17d417de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae888d99-5921-452c-b460-ec821c78d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirs = {\n",
    "'dnabert':data_dir + 'models/dnabert/6-new-12w-0/',\n",
    "'dnabert2':data_dir + 'models/dnabert2/DNABERT-2-117M/',\n",
    "'ntrans-v2-250m':data_dir + 'models/nucleotide-transformer-v2-250m-multi-species',\n",
    "'ntrans-v2-500m':data_dir + 'models/nucleotide-transformer-v2-500m-multi-species',\n",
    "'dnabert-3utr':data_dir + 'models/dnabert-3utr/checkpoints/epoch_30/',\n",
    "'dnabert2-3utr':data_dir + 'models/dnabert2-3utr/checkpoints/epoch_18/',\n",
    "'ntrans-v2-250m-3utr':data_dir + 'models/ntrans-v2-250m-3utr/checkpoints/epoch_23/'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c705abfb-e899-453f-9fd3-358e16552fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_params = misc.dotdict({})\n",
    "\n",
    "input_params.fasta = data_dir + '/fasta/Homo_sapiens_rna.fa'\n",
    "\n",
    "input_params.model = 'dnabert'\n",
    "\n",
    "input_params.output_dir = './test/'\n",
    "\n",
    "input_params.batch_size = 3\n",
    "\n",
    "input_params.include_txt = None \n",
    "\n",
    "input_params.N_folds = 10\n",
    "\n",
    "input_params.fold=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e25d78-2e9a-4e8e-8718-2989a1b3dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA device: CPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    cuda_device_name = torch.cuda.get_device_name(0)\n",
    "    print(f'\\nCUDA device: {cuda_device_name}\\n')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('\\nCUDA device: CPU\\n')\n",
    "    #raise Exception('CUDA is not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4219dd6f-6465-4fd9-9a91-770cdc6f5940",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = {'dnabert':510,'dnabert2':512,'ntrans-v2-250m':1024,'ntrans-v2-500m':1024,\n",
    "              'dnabert-3utr':510,'dnabert2-3utr':1024,'ntrans-v2-250m-3utr':1024,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc5ec02-25e8-4d27-b799-5c0a033b695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "\n",
    "    print(f'Loading model {model_name} from {model_dirs[model_name]}')\n",
    "\n",
    "    if  'dnabert' in model_name and not 'dnabert2' in model_name:\n",
    "        \n",
    "        tokenizer = DNATokenizer(vocab_file='./DNABERT/src/transformers/dnabert-config/bert-config-6/vocab.txt',max_len=510)\n",
    "        model = BertForMaskedLM.from_pretrained(model_dirs[model_name]).to(device);\n",
    "\n",
    "    elif 'dnabert2' in model_name:\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dirs[model_name],trust_remote_code=True)\n",
    "        embeddings_model = AutoModel.from_pretrained(model_dirs[model_name],trust_remote_code=True).to(device);\n",
    "        prediction_model = BertForMaskedLM.from_pretrained(model_dirs[model_name]).to(device);\n",
    "        model = (embeddings_model, prediction_model)\n",
    "    \n",
    "    elif 'ntrans' in model_name:\n",
    "\n",
    "        # Import the tokenizer and the model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dirs[model_name],trust_remote_code=True)\n",
    "        model = AutoModelForMaskedLM.from_pretrained(model_dirs[model_name],trust_remote_code=True).to(device);\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8e253c-5b44-41f4-9571-8c344d71412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, fasta_file, fold=None, N_folds=None, include_txt=None):\n",
    "        \n",
    "        seqs = defaultdict(str)\n",
    "            \n",
    "        with open(fasta_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('>'):\n",
    "                    transcript_id = line[1:].rstrip()\n",
    "                else:\n",
    "                    seqs[transcript_id] += line.rstrip().upper()\n",
    "                    \n",
    "        #seqs = {k:v[:MAX_SEQ_LENGTH] for k,v in seqs.items()}\n",
    "        #seqs = {k:''.join(np.random.choice(list('ACGT'),size=MAX_LENGTH)) for k,v in seqs.items()}\n",
    "        seqs = list(seqs.items())\n",
    "\n",
    "        if include_txt!=None:\n",
    "            print(f'Including sequences from {include_txt}')\n",
    "            processed_seqs = pd.read_csv(include_txt,names=['seq_name']).seq_name.values\n",
    "            seqs = [(seq_name,seq) for seq_name,seq in seqs if seq_name in processed_seqs]\n",
    "        if N_folds!=None:\n",
    "            print(f'Fold {fold}')\n",
    "            folds = np.tile(np.arange(N_folds),len(seqs)//N_folds+1)[:len(seqs)]\n",
    "            seqs = [x for idx,x in enumerate(seqs) if folds[idx]==fold]\n",
    "            \n",
    "        self.seqs = seqs\n",
    "        self.max_length = max([len(seq[1]) for seq in self.seqs])\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.seqs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.seqs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c1342da-3f5b-43eb-9f20-d10d7a22563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmers_stride1(seq, k=6):\n",
    "    # splits a sequence into overlapping k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq)-k+1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34741242-a34f-40f2-bad9-9c53f7d1ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_embeddings(model_name, sequences):\n",
    "\n",
    "    if input_params.max_tokenized_length is None:\n",
    "        max_tokenized_length = max_length[model_name]\n",
    "    else:\n",
    "        max_tokenized_length = input_params.max_tokenized_length\n",
    "\n",
    "    if 'dnabert' in model_name and not 'dnabert2' in model_name:\n",
    "\n",
    "        mean_sequence_embeddings = []\n",
    "        losses = []\n",
    "\n",
    "        #special_token_ids = [tokenizer.pad_token_id, tokenizer.mask_token_id, tokenizer.sep_token_id, tokenizer.cls_token_id, tokenizer.unk_token_id]\n",
    "        \n",
    "        for seq in sequences:\n",
    "\n",
    "            if len(seq)<6:\n",
    "                emb_seq = np.zeros((1,768))\n",
    "                emb_seq[:] = np.nan\n",
    "                losses.append(np.nan)\n",
    "                mean_sequence_embeddings.append(emb_seq)\n",
    "                continue\n",
    "\n",
    "            if len(seq)>max_tokenized_length:\n",
    "                warnings.warn('Cutting out the central part of the sequence to center around DNABERT FOV')\n",
    "                seq = misc.center_seq(seq,max_tokenized_length)\n",
    "\n",
    "            seq_kmer = kmers_stride1(seq)\n",
    "        \n",
    "            tokenized_seq = tokenizer.encode_plus(seq_kmer,\n",
    "                                                truncation = True,\n",
    "                                                return_tensors = 'pt',  \n",
    "                                                add_special_tokens=True, \n",
    "                                                max_length=512)[\"input_ids\"]\n",
    "            \n",
    "            torch_outs = model(tokenized_seq.to(device), \n",
    "                                labels = tokenized_seq.to(device),\n",
    "                                output_hidden_states=True)\n",
    "                \n",
    "            emb_seq = torch_outs.hidden_states[-1].mean(dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            mean_sequence_embeddings.append(emb_seq)\n",
    "\n",
    "            losses.append(torch_outs.loss.item())\n",
    "            \n",
    "            #probas = F.softmax(torch_outs['logits'],dim=2).detach().cpu().numpy()\n",
    "            #token_ids = model_input.cpu().numpy()\n",
    "            #gt_probas = np.take_along_axis(probas, token_ids[...,None], axis=2)\n",
    "            #gt_probas = gt_probas[~np.isin(token_ids,special_token_ids)]\n",
    "            #log_probas_seq = np.log(gt_probas).squeeze()\n",
    "            #log_probas.append(log_probas_seq)\n",
    "\n",
    "        return (np.vstack(mean_sequence_embeddings),np.array(losses))\n",
    "\n",
    "    elif 'dnabert2' in model_name:\n",
    "            \n",
    "        inputs = tokenizer(sequences, \n",
    "                           truncation=True, \n",
    "                           return_tensors = 'pt', \n",
    "                           padding=\"max_length\", \n",
    "                           max_length = max_tokenized_length).to(device)\n",
    "        \n",
    "        assert 'A100' in cuda_device_name, 'A100 GPU is required to generate embeddings for the DNABERT2 model'\n",
    "            \n",
    "        hidden_states = model[0](inputs[\"input_ids\"],\n",
    "                                attention_mask=inputs[\"attention_mask\"])[0] # [1, sequence_length, 768]\n",
    "        attention_mask = torch.unsqueeze(inputs['attention_mask'], dim=-1)\n",
    "        \n",
    "        # Compute mean embeddings per sequence\n",
    "        mean_sequence_embeddings = torch.sum(attention_mask*hidden_states, axis=-2)/torch.sum(attention_mask, axis=1)\n",
    "        \n",
    "        #mean_sequence_embeddings = torch.mean(hidden_states, dim=1)# embedding with mean pooling\n",
    "        \n",
    "        torch_outs = model[1](inputs[\"input_ids\"],\n",
    "                            attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "        losses = F.cross_entropy(torch_outs.logits.reshape((-1,torch_outs.logits.shape[-1])), \n",
    "                      inputs[\"input_ids\"].reshape(-1), reduction='none').reshape((len(sequences),-1)).sum(1)\n",
    "        \n",
    "        losses = losses.detach().cpu().numpy()\n",
    "        mean_sequence_embeddings = mean_sequence_embeddings.detach().cpu().numpy()\n",
    "\n",
    "        return (mean_sequence_embeddings, losses)\n",
    "\n",
    "    elif 'ntrans' in model_name:\n",
    "\n",
    "        inputs = tokenizer.batch_encode_plus(sequences, \n",
    "                                                      truncation = True,\n",
    "                                                      return_tensors=\"pt\", \n",
    "                                                      padding=\"max_length\", \n",
    "                                                      max_length = max_tokenized_length).to(device)\n",
    "                    \n",
    "        torch_outs = model(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            encoder_attention_mask=inputs['attention_mask'],\n",
    "            output_hidden_states=True)\n",
    "        \n",
    "        # Compute sequences embeddings\n",
    "        embeddings = torch_outs['hidden_states'][-1]\n",
    "        #print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        #print(f\"Embeddings per token: {embeddings}\")\n",
    "        \n",
    "        # Add embed dimension axis\n",
    "        attention_mask = torch.unsqueeze(inputs['attention_mask'], dim=-1)\n",
    "        \n",
    "        # Compute mean embeddings per sequence\n",
    "        mean_sequence_embeddings = torch.sum(attention_mask*embeddings, axis=-2)/torch.sum(attention_mask, axis=1)\n",
    "        #print(f\"Mean sequence embeddings: {mean_sequence_embeddings}\")\n",
    "\n",
    "        #probas = F.softmax(torch_outs['logits'],dim=2).cpu().numpy()\n",
    "        #inputs = inputs.cpu().numpy()\n",
    "        #gt_probas = np.take_along_axis(probas, inputs[...,None], axis=2).squeeze()\n",
    "        #log_probas = np.log(gt_probas)\n",
    "        \n",
    "        losses = F.cross_entropy(torch_outs.logits.reshape((-1,torch_outs.logits.shape[-1])), \n",
    "                      inputs[\"input_ids\"].reshape(-1), reduction='none').reshape((len(sequences),-1)).sum(1)\n",
    "        \n",
    "        mean_sequence_embeddings = mean_sequence_embeddings.detach().cpu().numpy()\n",
    "        losses = losses.detach().cpu().numpy()\n",
    "\n",
    "    return (mean_sequence_embeddings, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0393cf6-9c9e-4757-ac4e-26820ca1fe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model dnabert from /lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/dnabert/6-new-12w-0/\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = load_model(input_params.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e06a4f73-3731-405b-a909-1dc8ec8d112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    }
   ],
   "source": [
    "dataset = SeqDataset(input_params.fasta, fold=input_params.fold, N_folds=input_params.N_folds, include_txt=input_params.include_txt)\n",
    "\n",
    "dataloader = DataLoader(dataset = dataset, \n",
    "                        batch_size = input_params.batch_size, \n",
    "                        num_workers = 2, collate_fn = None, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9bfa6f2-a237-4a38-b45f-b10fbb9ffdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model_name in ('dnabert','dnabert2', 'ntrans-v2-250m', 'dnabert-3utr','dnabert2-3utr', 'ntrans-v2-250m-3utr'):\n",
    "#    print(model_name)\n",
    "#    tokenizer, model = load_model(model_name)\n",
    "#    seq_names,sequences = next(iter(dataloader))\n",
    "#    embeddings, losses = get_batch_embeddings(model_name,sequences)\n",
    "#    print(model, embeddings.shape, losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a793f585-c140-4edc-a006-44a57bea7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq = ''.join(np.random.choice(list('ACGT'),size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1258f891-1a64-4a39-af13-75c9722af6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "generating embeddings for batch 0/605\n",
      "generating embeddings for batch 1/605\n",
      "generating embeddings for batch 2/605\n",
      "generating embeddings for batch 3/605\n",
      "generating embeddings for batch 4/605\n",
      "generating embeddings for batch 5/605\n",
      "generating embeddings for batch 6/605\n",
      "generating embeddings for batch 7/605\n",
      "generating embeddings for batch 8/605\n",
      "generating embeddings for batch 9/605\n",
      "generating embeddings for batch 10/605\n",
      "generating embeddings for batch 11/605\n",
      "generating embeddings for batch 12/605\n",
      "generating embeddings for batch 13/605\n",
      "generating embeddings for batch 14/605\n",
      "generating embeddings for batch 15/605\n",
      "generating embeddings for batch 16/605\n",
      "generating embeddings for batch 17/605\n",
      "generating embeddings for batch 18/605\n",
      "generating embeddings for batch 19/605\n",
      "generating embeddings for batch 20/605\n",
      "generating embeddings for batch 21/605\n",
      "generating embeddings for batch 22/605\n",
      "generating embeddings for batch 23/605\n",
      "generating embeddings for batch 24/605\n",
      "generating embeddings for batch 25/605\n",
      "generating embeddings for batch 26/605\n",
      "generating embeddings for batch 27/605\n",
      "generating embeddings for batch 28/605\n",
      "generating embeddings for batch 29/605\n",
      "generating embeddings for batch 30/605\n",
      "generating embeddings for batch 31/605\n",
      "generating embeddings for batch 32/605\n",
      "generating embeddings for batch 33/605\n",
      "generating embeddings for batch 34/605\n",
      "generating embeddings for batch 35/605\n",
      "generating embeddings for batch 36/605\n",
      "generating embeddings for batch 37/605\n",
      "generating embeddings for batch 38/605\n",
      "generating embeddings for batch 39/605\n",
      "generating embeddings for batch 40/605\n",
      "generating embeddings for batch 41/605\n",
      "generating embeddings for batch 42/605\n",
      "generating embeddings for batch 43/605\n",
      "generating embeddings for batch 44/605\n",
      "generating embeddings for batch 45/605\n",
      "generating embeddings for batch 46/605\n",
      "generating embeddings for batch 47/605\n",
      "generating embeddings for batch 48/605\n",
      "generating embeddings for batch 49/605\n",
      "generating embeddings for batch 50/605\n",
      "generating embeddings for batch 51/605\n",
      "generating embeddings for batch 52/605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_emb, all_losses = [], []\n",
    "\n",
    "for seq_idx, (seq_names,sequences) in enumerate(dataloader):\n",
    "\n",
    "    #if os.path.isfile(input_params.output_dir + f'/{seq_names[0]}.pickle'):\n",
    "    #    continue\n",
    "        \n",
    "    print(f'generating embeddings for batch {seq_idx}/{len(dataloader)}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings, losses = get_batch_embeddings(input_params.model,sequences)\n",
    "\n",
    "    all_emb.append(embeddings)\n",
    "    all_losses.append(losses)\n",
    "\n",
    "    #with open(input_params.output_dir + f'/{seq_names[0]}.pickle', 'wb') as f:\n",
    "    #    pickle.dump((seq_names,emb,logprobs),f)\n",
    "\n",
    "if input_params.fold!=None:\n",
    "    output_name = input_params.output_dir + f'/predictions_{input_params.fold}.pickle'\n",
    "else:\n",
    "    output_name = input_params.output_dir + '/predictions.pickle'\n",
    "\n",
    "os.makedirs(input_params.output_dir, exist_ok=True)\n",
    "\n",
    "seq_names, seqs = zip(*dataset.seqs)\n",
    "\n",
    "with open(output_name, 'wb') as f:\n",
    "    pickle.dump({'seq_names':seq_names, 'seqs':seqs, 'embeddings':np.vstack(all_emb), \n",
    "                 'losses':np.hstack(all_losses), 'fasta':input_params.fasta},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7afafeb8-85a4-4175-b7e0-c18e3ff7cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + '/variants/embeddings/dnabert2/predictions.pickle','rb') as fin:\n",
    "    data = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b38b2-a03e-4391-bb0f-49cb4584a554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
