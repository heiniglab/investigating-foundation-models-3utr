{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaef155c-efca-43c0-8f0f-e18f33571886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/sergey.vilov/miniconda3/envs/mlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import os,sys\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01214a05-9e3b-4764-8663-5087ce322f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM,AutoModel\n",
    "\n",
    "sys.path.append('DNABERT/')\n",
    "\n",
    "from src.transformers import DNATokenizer \n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a39aea0-7515-427b-b3cd-a99631fb564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    '''\n",
    "    dot.notation access to dictionary attributes\n",
    "    '''\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7440f12c-8fe4-435c-842a-ba17d417de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/MLM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c705abfb-e899-453f-9fd3-358e16552fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_params = dotdict({})\n",
    "\n",
    "#input_params.fasta = data_dir + 'griesemer/fasta/GRCh38_UTR_variants.fa'\n",
    "input_params.fasta = data_dir + 'fasta/Homo_sapiens_no_reverse.fa'\n",
    "\n",
    "input_params.model = 'NT-MS-v2-500M'\n",
    "\n",
    "input_params.output_dir = data_dir + f'griesemer/embeddings/{input_params.model}/'\n",
    "\n",
    "input_params.batch_size = 10\n",
    "\n",
    "input_params.processed_seqs = data_dir + f'/3UTR_embeddings/{input_params.model}/processed_utrs.csv'\n",
    "\n",
    "input_params.N_folds = 10\n",
    "\n",
    "input_params.fold=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410df6ea-f4ed-4e6c-b4e7-61dc8c2a5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bc5ec02-25e8-4d27-b799-5c0a033b695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    \n",
    "    model_dirs = {'DNABERT':data_dir + 'dnabert/default/6-new-12w-0/',\n",
    "                  'DNABERT-2':data_dir + 'dnabert2/DNABERT-2-117M/',\n",
    "                  'NT-MS-v2-500M':data_dir + 'nucleotide-transform/nucleotide-transformer-v2-500m-multi-species'} \n",
    "\n",
    "    if model_name == 'DNABERT':\n",
    "        \n",
    "        config = BertConfig.from_pretrained('https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/config.json')\n",
    "        tokenizer = DNATokenizer.from_pretrained('dna6')\n",
    "        model = BertModel.from_pretrained(model_dirs[model_name], config=config)\n",
    "\n",
    "    elif model_name == 'DNABERT-2':\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dirs[model_name],trust_remote_code=True)\n",
    "        model = AutoModel.from_pretrained(model_dirs[model_name],trust_remote_code=True)\n",
    "\n",
    "    elif model_name == 'NT-MS-v2-500M':\n",
    "\n",
    "        # Import the tokenizer and the model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dirs[model_name],trust_remote_code=True)\n",
    "        model = AutoModelForMaskedLM.from_pretrained(model_dirs[model_name],trust_remote_code=True)\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94da8979-2d48-4b73-9549-4dee2422e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from glob import glob\n",
    "#import pickle\n",
    "#\n",
    "#processed_seqs = []\n",
    "#for emb_file in glob(data_dir + f'/3UTR_embeddings/{input_params.model}/ENST*.pickle'):\n",
    "#    with open(emb_file, 'rb') as f:\n",
    "#        utr_names_batch, _ = pickle.load(f)\n",
    "#        processed_seqs.extend(utr_names_batch)\n",
    "#\n",
    "#pd.Series(processed_seqs).to_csv(data_dir + f'/3UTR_embeddings/{input_params.model}/processed_utrs.csv',index=None,header=none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8e253c-5b44-41f4-9571-8c344d71412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, fasta_file):\n",
    "        \n",
    "        seqs = defaultdict(str)\n",
    "            \n",
    "        with open(fasta_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('>'):\n",
    "                    transcript_id = line[1:].split(':')[0].rstrip()\n",
    "                else:\n",
    "                    seqs[transcript_id] += line.rstrip().upper()\n",
    "                    \n",
    "        seqs = {k:v[:MAX_SEQ_LENGTH] for k,v in seqs.items()}\n",
    "        #seqs = {k:''.join(np.random.choice(list('ACGT'),size=MAX_LENGTH)) for k,v in seqs.items()}\n",
    "        seqs = list(seqs.items())\n",
    "\n",
    "        if input_params.exclude!=None:\n",
    "            print(f'Excluding sequences from {input_params.processed_seqs}')\n",
    "            processed_seqs = pd.read_csv(input_params.exclude,names=['seq_name']).seq_name.values\n",
    "            seqs = [(seq_name,seq) for seq_name,seq in seqs if not seq_name in processed_seqs]\n",
    "        if input_params.N_folds!=None:\n",
    "            print(f'Fold {input_params.fold}')\n",
    "            folds = np.tile(np.arange(input_params.N_folds),len(seqs)//input_params.N_folds+1)[:len(seqs)]\n",
    "            seqs = [x for idx,x in enumerate(seqs) if folds[idx]==input_params.fold]\n",
    "            \n",
    "        self.seqs = seqs\n",
    "        self.max_length = max([len(seq[1]) for seq in self.seqs])\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.seqs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.seqs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34741242-a34f-40f2-bad9-9c53f7d1ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmers_stride1(seq, k=6):\n",
    "    # splits a sequence into overlapping k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq)-k+1)] \n",
    "\n",
    "\n",
    "def get_batch_embeddings(model_name, sequences):\n",
    "\n",
    "    if model_name == 'DNABERT':\n",
    "\n",
    "        outputs = []\n",
    " \n",
    "        for seq in sequences:\n",
    "\n",
    "            seq_kmer = kmers_stride1(seq)\n",
    "    \n",
    "            model_input = tokenizer.encode_plus(seq_kmer, add_special_tokens=True, padding='max_length', max_length=512)[\"input_ids\"]\n",
    "            model_input = torch.tensor(model_input, dtype=torch.long)\n",
    "            model_input = model_input.unsqueeze(0)   # to generate a fake batch with batch size one\n",
    "\n",
    "            output = model(model_input)\n",
    "            outputs.append(output[1])\n",
    "\n",
    "        return torch.vstack(outputs)\n",
    "\n",
    "    elif model_name == 'DNABERT-2':\n",
    "\n",
    "        inputs = tokenizer(sequences, return_tensors = 'pt', padding=\"max_length\", max_length = dataset.max_length)[\"input_ids\"]\n",
    "        \n",
    "        hidden_states = model(inputs)[0] # [1, sequence_length, 768]\n",
    "        \n",
    "        # embedding with mean pooling\n",
    "        mean_sequence_embeddings = torch.mean(hidden_states, dim=1)\n",
    "\n",
    "        return mean_sequence_embeddings\n",
    "\n",
    "    elif model_name == 'NT-MS-v2-500M':\n",
    "\n",
    "        batch_token_ids = tokenizer.batch_encode_plus(sequences, return_tensors=\"pt\", padding=\"max_length\", max_length = dataset.max_length)[\"input_ids\"]\n",
    "\n",
    "        attention_mask = batch_token_ids != tokenizer.pad_token_id\n",
    "            \n",
    "        torch_outs = model(\n",
    "            batch_token_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            output_hidden_states=True)\n",
    "        \n",
    "        # Compute sequences embeddings\n",
    "        embeddings = torch_outs['hidden_states'][-1].detach().numpy()\n",
    "        #print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        #print(f\"Embeddings per token: {embeddings}\")\n",
    "        \n",
    "        # Add embed dimension axis\n",
    "        attention_mask = torch.unsqueeze(attention_mask, dim=-1)\n",
    "        \n",
    "        # Compute mean embeddings per sequence\n",
    "        mean_sequence_embeddings = torch.sum(attention_mask*embeddings, axis=-2)/torch.sum(attention_mask, axis=1)\n",
    "        #print(f\"Mean sequence embeddings: {mean_sequence_embeddings}\")\n",
    "\n",
    "        probas = F.softmax(torch_outs['logits'],dim=2).cpu().numpy()\n",
    "        batch_token_ids = batch_token_ids.cpu().numpy()\n",
    "        gt_probas = np.take_along_axis(probas, batch_token_ids[...,None], axis=2).squeeze()\n",
    "        log_probas = np.log(gt_probas)\n",
    "\n",
    "    return (mean_sequence_embeddings, log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cde01186-278b-4259-a386-f8fa9e36b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0393cf6-9c9e-4757-ac4e-26820ca1fe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/sergey.vilov/miniconda3/envs/ntrans/lib/python3.10/site-packages/transformers/utils/hub.py:575: UserWarning: Using `from_pretrained` with the url of a file (here https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/config.json) is deprecated and won't be possible anymore in v5 of Transformers. You should host your file on the Hub (hf.co) instead and use the repository ID. Note that this is not compatible with the caching system (your file will be downloaded at each execution) or multiple processes (each process will download the file in a different temporary file).\n",
      "  warnings.warn(\n",
      "Downloading (…)config-6/config.json: 359B [00:00, 9.92kB/s]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'src.transformers.tokenization_dna.DNATokenizer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /lustre/groups/epigenereg01/workspace/projects/vale/MLM/dnabert/default/6-new-12w-0/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = load_model(input_params.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e06a4f73-3731-405b-a909-1dc8ec8d112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SeqDataset(input_params.fasta)\n",
    "\n",
    "dataloader = DataLoader(dataset = dataset, \n",
    "                        batch_size = input_params.batch_size, \n",
    "                        num_workers = 2, collate_fn = None, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258f891-1a64-4a39-af13-75c9722af6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emb = []\n",
    "\n",
    "for seq_idx, (seq_names,sequences) in enumerate(dataloader):\n",
    "\n",
    "    print(f'generating embeddings for batch {seq_idx}/{len(dataloader)}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = get_batch_embeddings(input_params.model,sequences).cpu().numpy()\n",
    "\n",
    "    all_emb.append(emb)\n",
    "\n",
    "all_emb = np.vstack(all_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "336d3387-a995-4e90-ac3e-398cafb52eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(input_params.output_dir, exist_ok=True)\n",
    "\n",
    "with open(input_params.output_dir + 'embeddings.npy', 'wb') as f:\n",
    "    np.save(f, all_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b2503-02c9-462a-a3ce-49d24afea1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
