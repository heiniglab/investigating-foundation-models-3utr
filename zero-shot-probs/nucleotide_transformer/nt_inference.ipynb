{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaef155c-efca-43c0-8f0f-e18f33571886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283206cf-5075-40c0-8b39-62b4cf4c14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 10\n",
    "MAX_TOK_LEN = 1024\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d13d37e-9982-445e-a887-53d6155e8c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKING = True\n",
    "DECODE=None#'reference-aware'\n",
    "CENTRAL_WINDOW=None\n",
    "reverse_seq_neg_strand = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4b3f4e-5e95-4a93-9271-f77037fe0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7247644-bed7-424d-a4e8-146bcba7d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/whole_genome/nucleotide-transformer-v2-100m-multi-species'\n",
    "\n",
    "#model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/zoonomia-3utr/ntrans-v2-100m-3utr-2e/checkpoints/chkpt_600/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b8df946-4917-441b-ad30-e37c261ce89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir,trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_dir,trust_remote_code=True).to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62901dc9-228d-489f-a423-756e737d3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(seq_tokens):\n",
    "    '''\n",
    "    Chunk the given token sequence into chunks of MAX_TOK_LEN\n",
    "    The input sequences shouldn't contain special tokens\n",
    "    The last chunk is padded with the previous chunk if it's shorter than MAX_TOK_LEN\n",
    "    '''\n",
    "    if tokenizer.eos_token_id:\n",
    "        #in the original InstaDeep models, the cls token wasn't present\n",
    "        chunk_len = MAX_TOK_LEN-2 #2 special tokens to be added \n",
    "    else:\n",
    "        chunk_len = MAX_TOK_LEN-1 #only cls token\n",
    "    chunks = [seq_tokens[start:start+chunk_len] for start in range(0,len(seq_tokens),chunk_len)]\n",
    "    assert [x for y in chunks for x in y]==seq_tokens\n",
    "    if len(chunks)>1:\n",
    "        left_shift = min(chunk_len-len(chunks[-1]), len(chunks[-2])) #overlap length for the last chunk and the previous one\n",
    "        if left_shift>0:\n",
    "            pad_seq = chunks[-2][-left_shift:]\n",
    "            chunks[-1] = pad_seq + chunks[-1]\n",
    "    else:\n",
    "        left_shift = 0\n",
    "    if tokenizer.eos_token_id:\n",
    "        chunks = [[tokenizer.cls_token_id, *chunk, tokenizer.eos_token_id] for chunk in chunks]\n",
    "        assert [x for y in chunks[:-1] for x in y[1:-1]]+[x for x in  chunks[-1][1+left_shift:-1]]==seq_tokens\n",
    "    else:\n",
    "        chunks = [[tokenizer.cls_token_id, *chunk] for chunk in chunks]\n",
    "        assert [x for y in chunks[:-1] for x in y[1:]]+[x for x in  chunks[-1][1+left_shift:]]==seq_tokens\n",
    "    #left_shift only makes sense for the last chunk, for the other chunks it's 0\n",
    "    res = [(chunk,0) if chunk_idx!=len(chunks)-1 else (chunk,left_shift) for chunk_idx, chunk in enumerate(chunks)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc320d73-2e97-4407-91bb-f02f1f223977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_sequence(seq_tokens, mask_crop_left=0,mask_crop_right=None):\n",
    "    '''\n",
    "    Consecutively mask tokens in the sequence and yield each masked position\n",
    "    Mask tokens between mask_crop_left and mask_crop_right\n",
    "    Don't mask special tokens\n",
    "    '''    \n",
    "    if not mask_crop_right:\n",
    "        mask_crop_right = len(seq_tokens)-1\n",
    "    for mask_pos in range(1+mask_crop_left,1+mask_crop_right):\n",
    "        if seq_tokens[mask_pos] in (tokenizer.eos_token_id,tokenizer.pad_token_id):\n",
    "            break\n",
    "        masked_seq = seq_tokens.clone()\n",
    "        masked_seq[mask_pos] = tokenizer.mask_token_id\n",
    "        yield mask_pos, masked_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c1097eb-3236-404d-b076-fd948f3170b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, seq_df, masking=True):\n",
    "        \n",
    "        self.seq_df = seq_df\n",
    "        self.start = 0\n",
    "        self.end = len(self.seq_df)\n",
    "        self.masking = masking\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        for seq_idx in range(self.start, self.end):\n",
    "            \n",
    "            seq_info = self.seq_df.iloc[seq_idx]\n",
    "            chunk = seq_info.tokens\n",
    "            \n",
    "            gt_tokens = torch.LongTensor(chunk)\n",
    "\n",
    "            mask_crop_left = seq_info.crop_mask_left\n",
    "            mask_crop_right = seq_info.crop_mask_right\n",
    "                \n",
    "            if self.masking:\n",
    "                for masked_pos, masked_tokens in mask_sequence(gt_tokens, mask_crop_left, mask_crop_right):\n",
    "                    #consecutively mask each token in the sequence\n",
    "                    assert masked_tokens[masked_pos] == tokenizer.mask_token_id\n",
    "                    yield seq_info.name, gt_tokens, masked_pos, masked_tokens\n",
    "            else:\n",
    "                yield seq_info.name, gt_tokens, -1, gt_tokens\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "     worker_info = torch.utils.data.get_worker_info()\n",
    "     dataset = worker_info.dataset  # the dataset copy in this worker process\n",
    "     overall_start = dataset.start\n",
    "     overall_end = dataset.end\n",
    "     # configure the dataset to only process the split workload\n",
    "     per_worker = int(np.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
    "     worker_id = worker_info.id\n",
    "     dataset.start = overall_start + worker_id * per_worker\n",
    "     dataset.end = min(dataset.start + per_worker, overall_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a77604-3bc3-4383-9d1b-767c8cbbfe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_batch(masked_tokens_batch):\n",
    "\n",
    "    targets_masked = masked_tokens_batch.clone()\n",
    "    targets_masked[targets_masked!=tokenizer.mask_token_id] = -100\n",
    "    attention_mask = masked_tokens_batch!= tokenizer.pad_token_id   \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        torch_outs = model(\n",
    "        masked_tokens_batch.to(device),\n",
    "        labels = targets_masked.to(device),\n",
    "        attention_mask=attention_mask.to(device),\n",
    "        encoder_attention_mask=attention_mask.to(device),\n",
    "        output_hidden_states=False)\n",
    "    \n",
    "    logits = torch_outs[\"logits\"] #max_tokenized_length x (max_tokenized_length+1) x N_tokens\n",
    "    \n",
    "    probas_batch = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "    \n",
    "    loss = torch_outs[\"loss\"].item()\n",
    "    \n",
    "    return probas_batch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cd367b8-dbec-40ef-82a2-c1ac6484734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_complement(seq):\n",
    "    '''\n",
    "    Take sequence reverse complement\n",
    "    '''\n",
    "    compl_dict = {'A':'T', 'C':'G', 'G':'C', 'T':'A'}\n",
    "    compl_seq = ''.join([compl_dict.get(x,x) for x in seq])\n",
    "    rev_seq = compl_seq[::-1]\n",
    "    return rev_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7662e08e-eca3-4f01-b6bc-ef8bb9e2d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_seq(seq,tokens):\n",
    "\n",
    "    L = len(seq)\n",
    "    central_pos = L//2\n",
    "\n",
    "    decoded_tokens = tokenizer.decode(tokens).split()\n",
    "    \n",
    "    nt_idx = []\n",
    "    for token_idx,token in enumerate(decoded_tokens[1:]):\n",
    "        if not token.startswith('<'):\n",
    "            nt_idx.extend([token_idx]*len(token))\n",
    "    nt_idx = np.array(nt_idx)\n",
    "    crop_mask_left, crop_mask_right = nt_idx[central_pos],nt_idx[central_pos+CENTRAL_WINDOW]+1\n",
    "    seq_idx = np.where((nt_idx>=crop_mask_left) & (nt_idx<crop_mask_right))[0]\n",
    "    pos_left,pos_right = seq_idx[0], seq_idx[-1]+1\n",
    "    seq_cropped = seq[pos_left:pos_right]\n",
    "    assert seq_cropped.startswith(seq[pos_left:pos_left+CENTRAL_WINDOW])\n",
    "    return (seq, tokens, L, seq_cropped,pos_left, pos_right, crop_mask_left, crop_mask_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "963b87d9-b877-4898-90bb-de51e5266dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokendict_list = [{\"A\": [], \"G\": [], \"T\": [],\"C\": []} for x in range(6)]\n",
    "\n",
    "for tpl in itertools.product(\"ACGT\",repeat=6):\n",
    "    encoding = tokenizer.encode(\"\".join(tpl))\n",
    "    for idx, nuc in enumerate(tpl):\n",
    "        tokendict_list[idx][nuc].append(encoding[1]) #token indices for idx position in 6-mer and letter nuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58b41166-2b2e-40d3-8ca4-6ab8d94cd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/variants/selected/variants_rna.fa'\n",
    "#fasta = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/fasta/Homo_sapiens_dna_fwd.fa'\n",
    "\n",
    "seq_df = defaultdict(str)\n",
    "\n",
    "with open(fasta, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            seq_name = line[1:].rstrip()\n",
    "        else:\n",
    "            seq_df[seq_name] += line.rstrip()#.upper()\n",
    "            \n",
    "seq_df = pd.DataFrame(list(seq_df.items()), columns=['seq_name','seq']).set_index('seq_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19dc2de3-8648-4817-be72-911f1f378c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 10046 sequences\n"
     ]
    }
   ],
   "source": [
    "fold = 0\n",
    "\n",
    "folds = np.arange(N_FOLDS).repeat(len(seq_df)//N_FOLDS+1)[:len(seq_df)] #split into folds \n",
    "\n",
    "seq_df = seq_df.loc[folds==fold] #get required fold\n",
    "\n",
    "print(f'Fold {fold}: {len(seq_df)} sequences')\n",
    "\n",
    "original_seqs = seq_df.seq #sequences before tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4fe5b2f-3848-4dd1-a820-e76356422e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_ONLY_LOWERCASE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "025b1b3c-c113-4a30-9417-163819a732d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_seq(seq,tokens):\n",
    "\n",
    "    L = len(seq)\n",
    "    \n",
    "    if CENTRAL_WINDOW:\n",
    "        assert PREDICT_ONLY_LOWERCASE is None\n",
    "        left = L//2-CENTRAL_WINDOW//2\n",
    "        right = left+CENTRAL_WINDOW\n",
    "    elif PREDICT_ONLY_LOWERCASE:\n",
    "        lower_idx = np.array([idx for idx, c in enumerate(seq) if c.islower()])\n",
    "        left = lower_idx.min()\n",
    "        right = lower_idx.max()\n",
    "\n",
    "    decoded_tokens = tokenizer.decode(tokens).split()\n",
    "    \n",
    "    nt_idx = []\n",
    "    for token_idx,token in enumerate(decoded_tokens[1:]):\n",
    "        if not token.startswith('<'):\n",
    "            nt_idx.extend([token_idx]*len(token))\n",
    "            \n",
    "    nt_idx = np.array(nt_idx)\n",
    "    \n",
    "    crop_mask_left, crop_mask_right = nt_idx[left],nt_idx[right]+1\n",
    "    seq_idx = np.where((nt_idx>=crop_mask_left) & (nt_idx<crop_mask_right))[0]\n",
    "    \n",
    "    pos_left,pos_right = seq_idx[0], seq_idx[-1]+1\n",
    "    \n",
    "    seq_cropped = seq[pos_left:pos_right]\n",
    "    \n",
    "    assert seq_cropped.startswith(seq[pos_left:pos_left+right-left])\n",
    "    \n",
    "    return (seq, tokens, L, seq_cropped,pos_left, pos_right, crop_mask_left, crop_mask_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e93f9f3b-96b5-4b28-a154-55e1421e8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse complement on the negative strand if reverse_seq_neg_strand=True\n",
    "#strand_info = pd.read_csv(data_dir + 'UTR_coords/GRCh38_3_prime_UTR_clean-sorted.bed', sep='\\t', header = None, names=['seq_name','strand'], usecols=[3,5]).set_index('seq_name').squeeze()\n",
    "#seq_df.seq = seq_df.apply(lambda x: reverse_complement(x.seq) if strand_info.loc[x.seq_name]=='-' \n",
    "#                          and reverse_seq_neg_strand else x.seq, axis=1) #undo reverse complement\n",
    "\n",
    "\n",
    "if CENTRAL_WINDOW is not None or PREDICT_ONLY_LOWERCASE is not None:\n",
    "    seq_df['tokens'] = seq_df.seq.apply(lambda seq:tokenizer(seq.upper(),add_special_tokens=True)['input_ids'])\n",
    "    seq_df = pd.DataFrame([crop_seq(seq,tokens) for seq,tokens in seq_df.values], index=seq_df.index, columns=['seq','tokens','seq_length','seq_cropped','pos_left','pos_right','crop_mask_left','crop_mask_right'])\n",
    "else:\n",
    "    tokens = [(seq_name,chunk[0],chunk[1]) for seq_name,seq in seq_df.seq.items() for chunk in get_chunks(tokenizer(seq.upper(),add_special_tokens=False)['input_ids'])\n",
    "             ]\n",
    "    seq_df = pd.DataFrame(tokens,columns=['seq_name','tokens','crop_mask_left']).set_index('seq_name')\n",
    "    seq_df['crop_mask_right'] = seq_df.tokens.apply(len)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba0fbd82-ddeb-41a5-9c39-e7f65fdb747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    '''\n",
    "    Collate tokenized sequences based on the maximal sequence length in the batch\n",
    "    '''\n",
    "    seq_names_batch, gt_tokens_batch, masked_pos_batch, masked_tokens_batch = zip(*batch)\n",
    "    masked_tokens_batch = pad_sequence(masked_tokens_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    gt_tokens_batch = pad_sequence(gt_tokens_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return seq_names_batch, gt_tokens_batch, masked_pos_batch, masked_tokens_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "460ba37a-b224-450c-8799-83ec4b6f8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(SeqDataset(seq_df,masking=MASKING), batch_size=BATCH_SIZE, \n",
    "                        shuffle=False, collate_fn=collate_fn, \n",
    "                        num_workers=1, worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "659160bd-8ee7-4631-ad6e-421b9452ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probas_token(seq_token_probas,token_pos,gt_token):\n",
    "    '''\n",
    "    Predict probabilities of each bp for a given token\n",
    "    '''\n",
    "    seq_probas = []\n",
    "    for idx in range(len(gt_token)):\n",
    "        #loop over all positions of the masked token\n",
    "        position_probas = [] #probabilities for all bases at given position\n",
    "        for nuc in 'ACGT':\n",
    "            if DECODE=='reference-aware':\n",
    "                token_idx = tokenizer.token_to_id(gt_token[:idx]+nuc+gt_token[idx+1:]) #single token \n",
    "            else:\n",
    "                token_idx = tokendict_list[idx][nuc] #all tokens that have given base nuc at given position idx\n",
    "            position_probas.append(seq_token_probas[token_pos][token_idx].sum()) \n",
    "        seq_probas.append(position_probas)\n",
    "    return seq_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91a5089-0996-4065-8b10-7888122f878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 191/10046 [55:18<47:33:59, 17.38s/it]\n"
     ]
    }
   ],
   "source": [
    "nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3} #for accuracy\n",
    "\n",
    "all_probas = defaultdict(list) #probas for all masked tokens \n",
    "verif_seqs = defaultdict(str) #reconstruct sequences from mask tokens and make sure that they match the original sequences\n",
    "\n",
    "all_losses, is_correct = [], [] #to compute loss and accuracy\n",
    "prev_seq_name = None #name of the previous sequence\n",
    "\n",
    "pbar = tqdm(total=len(original_seqs))\n",
    "\n",
    "for seq_names_batch, gt_tokens_batch, masked_pos_batch, masked_tokens_batch in dataloader:\n",
    "\n",
    "    probas_batch, loss_batch = predict_on_batch(masked_tokens_batch)\n",
    "    #probas_batch, loss_batch = np.zeros((len(seq_names_batch),1024,4108)), 0 #placeholder for testing\n",
    "    \n",
    "    all_losses.append(loss_batch)\n",
    "    \n",
    "    for seq_name, gt_tokens, masked_pos, seq_probas in zip(seq_names_batch, gt_tokens_batch, masked_pos_batch, probas_batch):\n",
    "        gt_tokens = gt_tokens.cpu().tolist()\n",
    "        if MASKING:\n",
    "            gt_token = tokenizer.id_to_token(gt_tokens[masked_pos]) #ground truth masked token\n",
    "            all_probas[seq_name].extend(predict_probas_token(seq_probas,masked_pos,gt_token))\n",
    "            verif_seqs[seq_name] += gt_token\n",
    "        else:\n",
    "            for token_idx, gt_token in enumerate(gt_tokens):\n",
    "                gt_token = tokenizer.id_to_token(gt_token) #ground truth token\n",
    "                if not gt_token.startswith('<'):\n",
    "                    all_probas[seq_name].extend(predict_probas_token(seq_probas,token_idx,gt_token))\n",
    "                    verif_seqs[seq_name] += gt_token\n",
    "        if seq_name!=prev_seq_name:\n",
    "            #processing of prev_seq_name is completed\n",
    "            if len(verif_seqs[prev_seq_name])>0:\n",
    "                is_correct.extend([nuc_dict.get(base,4)==gt_idx for base, gt_idx in zip(verif_seqs[prev_seq_name],np.argmax(all_probas[prev_seq_name],axis=1))])\n",
    "                print(f'Sequence {prev_seq_name} processed ({len(verif_seqs)-1}/{len(original_seqs)}), loss: {np.mean(all_losses):.3}, acc:{np.mean(is_correct):.3}')\n",
    "                if CENTRAL_WINDOW or PREDICT_ONLY_LOWERCASE is not None:\n",
    "                    assert verif_seqs[prev_seq_name]==seq_df.loc[prev_seq_name]['seq_cropped'].upper() #compare reconstruction from the masked token with the original sequence\n",
    "                else:\n",
    "                    assert verif_seqs[prev_seq_name]==original_seqs.loc[prev_seq_name].upper() #compare reconstruction from the masked token with the original sequence\n",
    "                pbar.update(1)\n",
    "            prev_seq_name = seq_name\n",
    "              \n",
    "\n",
    "assert verif_seqs[seq_name]==original_seqs.loc[seq_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf206c-13a4-4c17-b824-ac5b642c36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_names = list(all_probas.keys())\n",
    "probs = [np.array(x) for x in all_probas.values()]\n",
    "seqs = original_seqs.loc[seq_names].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a275a-bdbc-47a3-82ec-c3c8db08ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CENTRAL_WINDOW or PREDICT_ONLY_LOWERCASE is not None:\n",
    "    for seq_idx,seq_name in enumerate(seq_names):\n",
    "        pad_left = np.ones((seq_df.loc[seq_name]['pos_left'],4))*0.25\n",
    "        pad_right = np.ones((seq_df.loc[seq_name]['seq_length']-seq_df.loc[seq_name]['pos_right'],4))*0.25\n",
    "        probs[seq_idx] = np.vstack((pad_left,probs[seq_idx],pad_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d0aeb-b34e-43b7-84b6-4f2af2c8f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_names = list(all_probas.keys())\n",
    "probs = [np.array(x) for x in all_probas.values()]\n",
    "seqs = original_seqs.loc[seq_names].values.tolist()\n",
    "\n",
    "if reverse_seq_neg_strand:\n",
    "    probs = [x[::-1,[3,2,1,0]] if strand_info.loc[seq_name]=='-' else x for x, seq_name in zip(probs,seq_names)]\n",
    "    seqs = [reverse_complement(x) if strand_info.loc[seq_name]=='-' else x for x, seq_name in zip(seqs,seq_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea19938-5603-4c33-8628-506e926ddb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(data_dir + f'motif_predictions/split_75_25/ntrans/NT-MS-v2-500M_{fold}.pickle', 'wb') as f:\n",
    "#    pickle.dump({'seq_names':seq_names,'seqs':seqs, 'probs':probs, 'fasta':fasta},f)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323f3d1-1f2f-469d-92cd-acc47d8e5764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
