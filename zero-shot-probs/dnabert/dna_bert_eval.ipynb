{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc9794c-c682-4f51-8295-36477b6c6c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig, BertForMaskedLM\n",
    "\n",
    "import torch \n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "from collections.abc import Mapping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971b3b99-0a57-451f-9f0a-3dd7793ceeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/'\n",
    "\n",
    "model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/whole_genome/dnabert/6-new-12w-0/'\n",
    "#model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/dnabert-3utr/checkpoints/epoch_30/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a55430e-7bee-4b2c-a7c5-1daf6b23de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5293e19d-3b08-48f3-9b57-d53c122e74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/dnabert-3utr/checkpoints/epoch_30/'\n",
    "#model_dir = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/models/dnabert/6-new-12w-0/'\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "#tokenizer.encode('GGGGGG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f131d-f465-4e23-8040-ce9585f069ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a93531-031b-4f59-a586-61d73eda558e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not '3utr' in model_dir:\n",
    "    #dna model: model trained on sequences that weren't reverse complemented for genes on negative strand\n",
    "    #all default DNABERT models\n",
    "    dna_model = True\n",
    "else:\n",
    "    dna_model = False\n",
    "\n",
    "dna_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447991ea-2c91-41a1-ad39-a992feb4ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a16e7-d43d-4817-83e9-09842ada9262",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32485ee-d112-4d0b-8d02-34049891cdaf",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa321a1c-e4a8-4efb-8bd0-65588c985d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "\n",
    "def chunkstring(string, length):\n",
    "    # chunks a string into segments of length\n",
    "    return (string[0+i:length+i] for i in range(0, len(string), length))\n",
    "\n",
    "def kmers(seq, k=6):\n",
    "    # splits a sequence into non-overlappnig k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq), k) if i + k <= len(seq)]\n",
    "\n",
    "def kmers_stride1(seq, k=6):\n",
    "    # splits a sequence into overlapping k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq)-k+1)]   \n",
    "\n",
    "def tok_func(x): return tokenizer(\" \".join(kmers_stride1(x[\"seq_chunked\"])))\n",
    "\n",
    "def one_hot_encode(gts, dim=5):\n",
    "    result = []\n",
    "    for nt in gts:\n",
    "        vec = np.zeros(dim)\n",
    "        vec[nuc_dict[nt]] = 1\n",
    "        result.append(vec)\n",
    "    return np.stack(result, axis=0)\n",
    "\n",
    "def class_label_gts(gts):\n",
    "    return np.array([nuc_dict[x] for x in gts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d63eb49f-0ab2-4f0c-8557-f476c4485ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers import  DataCollatorForLanguageModeling\n",
    "#data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability = 0.15)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of = None):\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple, np.ndarray)):\n",
    "        examples = [torch.tensor(e, dtype=torch.long) for e in examples]\n",
    "\n",
    "    length_of_first = examples[0].size(0)\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "\n",
    "    are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return torch.stack(examples, dim=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(x.size(0) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
    "    for i, example in enumerate(examples):\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            result[i, : example.shape[0]] = example\n",
    "        else:\n",
    "            result[i, -example.shape[0] :] = example\n",
    "    return result\n",
    "\n",
    "class DataCollatorForLanguageModelingSpan():\n",
    "    def __init__(self, tokenizer, mlm, mlm_probability, span_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm = mlm\n",
    "        self.span_length =span_length\n",
    "        self.mlm_probability= mlm_probability\n",
    "        self.pad_to_multiple_of = span_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n",
    "                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "            )\n",
    "        else:\n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def torch_mask_tokens(self, inputs, special_tokens_mask):\n",
    "        import torch\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability*0.2)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool().numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * self.span_length, mode = 'same' ),axis = 1, arr = masked_indices).astype(bool) \n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        m_save = masked_indices.clone()\n",
    "        \n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability*0.8) \n",
    "        probability_matrix.masked_fill_(masked_indices, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool().numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * self.span_length, mode = 'same' ),axis = 1, arr = masked_indices).astype(bool) \n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        m_final = masked_indices + m_save \n",
    "        labels[~m_final] = -100  # We only compute loss on masked tokens\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        #indices_replaced = torch.bernoulli(torch.full(labels.shape, 1.0)).bool()\n",
    "        #print (indices_replaced)\n",
    "        inputs[masked_indices] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        #print (masked_indices)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        #indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        #random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        #inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17bc4011-6add-4ce1-b56d-5f90f078f26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.kmers(seq, k=6)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a1c60e8-fba4-46e4-aadf-75b25cb44a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = np.random.choice(list('ACGT'), size=510)\n",
    "seq = ''.join(seq)\n",
    "\n",
    "kmers = kmers_stride1(seq)\n",
    "len(kmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "218f75fb-a2ad-4492-afc4-49601a741538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmers = kmers_stride1(seq)\n",
    "len(kmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5fa6ea95-7bdc-47b0-83f6-7d5b666e0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "central_pos = len(seq)//2\n",
    "central_token_idx = central_pos-5 #first token, where the central nucleotide appears\n",
    "window = 5\n",
    "token_idx_left,token_idx_right = central_token_idx,central_token_idx+window+6-1\n",
    "pos_left,pos_right = central_pos, central_pos+window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3e0cad0a-e59c-4a87-88ce-6b32babb766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmers[token_idx_left][-1] == seq[pos_left]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1c33e9a5-0082-4c24-a9ee-085e7254c40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmers[token_idx_right-1][0] == seq[pos_right-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b1cbc-11ab-4355-acb8-41a92d826060",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80d3c3a7-f9d8-4d48-aa48-19a1fca8c1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_batch(tokenized_data, dataset, seq_idx):\n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    label = dataset.iloc[seq_idx]['seq_chunked']\n",
    "    label_len = len(label)\n",
    "    if label_len < 6:\n",
    "        return torch.zeros(label_len,label_len,5), None\n",
    "    else:\n",
    "        if MASKING:\n",
    "            diag_matrix = torch.eye(tokenized_data['input_ids'].shape[1]).numpy()\n",
    "            masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * 6, mode = 'same' ),axis = 1, arr = diag_matrix).astype(bool)\n",
    "            masked_indices = torch.from_numpy(masked_indices)\n",
    "            masked_indices = masked_indices[3:label_len-5-2]\n",
    "            res = tokenized_data['input_ids'].expand(masked_indices.shape[0],-1).clone()\n",
    "            targets_masked = res.clone().to(device)\n",
    "            res[masked_indices] = 4\n",
    "            targets_masked[res!=4] = -100\n",
    "            #print (res[0], res.shape)\n",
    "        res = res.to(device)\n",
    "        with torch.no_grad():\n",
    "            model_outs = model(res,labels=targets_masked)\n",
    "            fin_calculation = torch.softmax(model_outs['logits'], dim=2).detach().cpu()   \n",
    "        return fin_calculation, model_outs['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9fe45-2c83-445f-b990-1f0dc8f7efbb",
   "metadata": {},
   "source": [
    "## Translating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a93927f-c4c5-4f4f-8452-5cbfdb022e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_prbs_from_pred(prediction, pred_pos, token_pos, label_pos, label):   \n",
    "    # pred_pos = \"kmer\" position in tokenized sequence (incl. special tokens)\n",
    "    # token_pos = position of nucleotide in kmer\n",
    "    # label_pos = position of actual nucleotide in sequence\n",
    "    model_pred = prediction\n",
    "    prbs = [torch.sum(model_pred[pred_pos,tokendict_list[token_pos][nuc]]) for nuc in [\"A\",\"C\",\"G\",\"T\"]]\n",
    "    gt = label[label_pos] # 6-CLS, zerobased\n",
    "    res = torch.tensor(prbs+[0.0])\n",
    "    return res, gt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca263894-392a-44f5-bb47-f0ec30eebbe9",
   "metadata": {},
   "source": [
    "# Prepare inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a469f-dc86-465a-81bc-3a7923631f3c",
   "metadata": {},
   "source": [
    "## Prepare dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e85d91d9-c486-47b0-a58d-4d8985ddf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_complement(seq):\n",
    "    '''\n",
    "    Take sequence reverse complement\n",
    "    '''\n",
    "    compl_dict = {'A':'T', 'C':'G', 'G':'C', 'T':'A'}\n",
    "    compl_seq = ''.join([compl_dict.get(x,x) for x in seq])\n",
    "    rev_seq = compl_seq[::-1]\n",
    "    return rev_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b634469c-a9ae-4bb2-8527-44d08387c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_folds = 10\n",
    "fold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d93af7b3-e99d-456a-89b9-3c505850a2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fasta = '/lustre/groups/epigenereg01/workspace/projects/vale/mlm/fasta/Homo_sapiens_rna.fa'\n",
    "\n",
    "dataset = defaultdict(str)\n",
    "\n",
    "with open(fasta, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            seq_name = line[1:].rstrip()\n",
    "        else:\n",
    "            dataset[seq_name] += line.rstrip().upper()\n",
    "            \n",
    "dataset = pd.DataFrame(list(dataset.items()), columns=['seq_name','seq'])\n",
    "\n",
    "folds = np.arange(N_folds).repeat(len(dataset)//N_folds+1)[:len(dataset)] \n",
    "\n",
    "dataset = dataset.loc[folds==fold]\n",
    "\n",
    "strand_info = pd.read_csv(data_dir + 'UTR_coords/GRCh38_3_prime_UTR_clean-sorted.bed', sep='\\t', header = None, names=['seq_name','strand'], usecols=[3,5]).set_index('seq_name').squeeze()\n",
    "\n",
    "dataset['original_seq'] = dataset['seq'] \n",
    "\n",
    "dataset['seq'] = dataset.apply(lambda x: reverse_complement(x.seq) if strand_info.loc[x.seq_name]=='-' and dna_model else x.seq, axis=1) #undo reverse complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "500a7c86-61d8-4a9b-b5ef-0fd3ad7e5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['seq'] = dataset.seq.apply(lambda x:''.join(np.random.choice(list(x),size=len(x),replace=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "599901ca-0b3a-44d8-9d9f-f380b702ed4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['seq_chunked'] = dataset['seq'].apply(lambda x : list(chunkstring(x, 510))) #chunk string in segments of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b57022c-c120-43e8-a76e-3eba91f4e209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#output_dir = '/s/project/mll/sergey/effect_prediction/MLM/motif_predictions/split_75_25/dnabert/default/'\n",
    "#\n",
    "#dataset_len = 500\n",
    "#\n",
    "#for dataset_start in range(0,len(dataset),dataset_len):\n",
    "#    df = dataset.iloc[dataset_start:dataset_start+dataset_len]\n",
    "#    df[['seq_name','seq']].to_csv(output_dir + f'/seq_{dataset_start}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2319883a-446c-4d6b-ac3e-ac87d87a5de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.explode('seq_chunked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "230e0d37-9326-4db7-8a16-76afc6247be9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32516f7a66f47d186f8e60dbb496cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/8359 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/sergey.vilov/miniconda3/envs/ntrans/lib/python3.10/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/icb/sergey.vilov/miniconda3/envs/ntrans/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(dataset[['seq_chunked']])\n",
    "\n",
    "tok_ds = ds.map(tok_func, batched=False,  num_proc=2)\n",
    "\n",
    "rem_tok_ds = tok_ds.remove_columns('seq_chunked')\n",
    "\n",
    "data_collator = DataCollatorForLanguageModelingSpan(tokenizer, mlm=False, mlm_probability = 0.025, span_length =6)\n",
    "data_loader = torch.utils.data.DataLoader(rem_tok_ds, batch_size=1, collate_fn=data_collator, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed132ad6-0aad-4cab-8dde-919719c02abc",
   "metadata": {},
   "source": [
    "## Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04f98016-880a-43c7-993c-35fe8fd5b69b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44910007-d73d-4499-83b3-2fec77d686be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "computed = []\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd982373-8c17-46c0-abed-7e1c72df1eed",
   "metadata": {},
   "source": [
    "## Prepare tokendict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a615b9f0-185f-4de2-a6fe-910cc6c992be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokendict_list = [{\"A\": [], \"G\": [], \"T\": [],\"C\": []} for x in range(6)]\n",
    "\n",
    "for tpl in itertools.product(\"ACGT\",repeat=6):\n",
    "    encoding = tokenizer.encode(\"\".join(tpl))\n",
    "    for idx, nuc in enumerate(tpl):\n",
    "        tokendict_list[idx][nuc].append(encoding[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a7a66-b2bf-4525-b2e5-ecff6ae664f1",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1be38a7-fc24-4015-9933-7902517f310b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "94974419-5a68-44d0-91dd-1f7e016ad63e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     model_outs \u001b[38;5;241m=\u001b[39m model(inputs,labels\u001b[38;5;241m=\u001b[39mtargets_masked)\n\u001b[1;32m      9\u001b[0m model_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(model_outs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241m.\u001b[39mappend(model_outs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "\n",
    "inputs = model_input_unaltered.clone().to(device) \n",
    "targets_masked = model_input_unaltered.clone().to(device)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    model_outs = model(inputs,labels=targets_masked)\n",
    "\n",
    "model_pred = torch.softmax(model_outs['logits'], dim=2).cpu().numpy()\n",
    "loss.append(model_outs['loss'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2925fe5-4ee5-4959-92af-7f7383d3fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_token = {v:k for k,v in tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfde9ed-977f-454a-8b9d-8d29a7674c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_seq = inputs[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d20d8a81-b486-427f-a3ec-7abc1181abfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G 0 ['GCAAAA'] range(0, -1, -1)\n",
      "C 1 ['GCAAAA', 'CAAAAA'] range(1, -1, -1)\n",
      "A 2 ['GCAAAA', 'CAAAAA', 'AAAAAT'] range(2, -1, -1)\n",
      "A 3 ['GCAAAA', 'CAAAAA', 'AAAAAT', 'AAAATA'] range(3, -1, -1)\n",
      "A 4 ['GCAAAA', 'CAAAAA', 'AAAAAT', 'AAAATA', 'AAATAG'] range(4, -1, -1)\n",
      "A 5 ['GCAAAA', 'CAAAAA', 'AAAAAT', 'AAAATA', 'AAATAG', 'AATAGT'] range(5, -1, -1)\n",
      "A 6 ['CAAAAA', 'AAAAAT', 'AAAATA', 'AAATAG', 'AATAGT', 'ATAGTG'] range(5, -1, -1)\n",
      "T 7 ['AAAAAT', 'AAAATA', 'AAATAG', 'AATAGT', 'ATAGTG', 'TAGTGT'] range(5, -1, -1)\n",
      "A 8 ['AAAATA', 'AAATAG', 'AATAGT', 'ATAGTG', 'TAGTGT', 'AGTGTG'] range(5, -1, -1)\n",
      "G 9 ['AAATAG', 'AATAGT', 'ATAGTG', 'TAGTGT', 'AGTGTG', 'GTGTGA'] range(5, -1, -1)\n"
     ]
    }
   ],
   "source": [
    "seq_batch_idx = 0\n",
    "seq_decoded = [id_to_token[token_id] for token_id in id_seq]\n",
    "seq = dataset.seq_chunked.iloc[0]\n",
    "\n",
    "seq_probas = []\n",
    "\n",
    "for pos_in_seq in range(len(seq)):\n",
    "    span_idx = slice(1+max(pos_in_seq-5,0),1+pos_in_seq+1)\n",
    "    span = seq_decoded[span_idx]\n",
    "    pos_in_token = range(len(span)-1,-1,-1)\n",
    "    span_pred = model_pred[seq_batch_idx][span_idx]\n",
    "    print(seq[pos_in_seq],pos_in_seq,span,pos_in_token,)\n",
    "    position_probas = []\n",
    "    for nuc in 'ACGT':\n",
    "        mut_span = [token[:pos]+nuc+token[pos+1:] for token,pos in zip(span,pos_in_token)]\n",
    "        mut_span_ids = [tokenizer.vocab[tok] for tok in mut_span]\n",
    "        position_probas.append(np.mean([span_pred[idx][mut_span_ids[idx]] for idx in range(len(span))]))\n",
    "    seq_probas.append(position_probas)\n",
    "    if pos_in_seq==9:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d58a6b67-c205-4065-abc3-0acc9c3be0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmap_dict = {0:'A',1:'C',2:'G',3:'T'}\n",
    "is_correct=[unmap_dict[i]==seq[idx] for idx,i in enumerate(np.argmax(seq_probas,1))]\n",
    "acc = np.mean(is_correct)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4d8e7d67-462d-4fcf-bf18-e887f1c87810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCTATCA'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = ''.join(np.random.choice(['A','C','G','T'],size=7))\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "65231dd0-ced7-4abe-b13a-e8d6ae3ed21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NNCCTATCAN'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "max_len = 10\n",
    "\n",
    "left = (len(seq)-max_len)//2\n",
    "\n",
    "if left>=0:\n",
    "    centered_seq = seq[left:left+max_len]\n",
    "else:\n",
    "    left = -left\n",
    "    right = max_len-len(seq)-left\n",
    "    centered_seq = 'N'*left + seq + 'N'*right\n",
    "\n",
    "centered_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "911239bb-223e-43dc-89ff-cd2c7db3bd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NNNNNNNNNN'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'N'*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1c140306-badb-4881-874a-0e577533fd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACCACACCAG'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "70e2e2fc-8fdb-4975-8e69-bc7150630116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACCACACCAGTGAACAAGAAGGAAAGGCCTTTTGGAGTGTGTCTTTCTGTGTGTTTAAAAACAGTGGGAAAATCCACACCACACTCTAAGTGGACAGCTCAGAATAATTTAGCATATTTCCTTTCACACTTAAAGTTCTTAAGATGAGACTGTGTAAATGAGAGAAAGACTTGATTCAGAGGAAAAAAATGTGTTCTCTCTGGACTCTTGCCTAGCTCACAAGGCTTTGGAGAATTGTCTTCCTAAATCGGGGCACCTGCTACAGAAGAGAATGTTTCGTTCCCTCTGGGTATGCACAGCTAAGGGGCCTCATTTCTCCCAGAGGGAGCTGCTGGCCCGCTCTAAGGGGTGCAAGAGGAAGGAGTGTGCCCAGACCAGCTGGGGGAGCATTGAAAGGACAGCTTGGGGATGGAATTTTTTTTTTTTTATCTCCATTTTCCAGTTAGGTAGAGCCAAGCTGAGGAATCCTTTTAAGATTATTAGAAAACATGCCCTGAAGAGAGTTGTTCT'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.seq_chunked.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "29f00660-95e0-4d15-b7d7-54c349eab162",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:06,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2278, 0.1440, 0.1964, 0.4317, 0.0000])\n",
      "chunks:1, acc:0.294, loss:3.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:13,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4078, 0.1677, 0.2584, 0.1661, 0.0000])\n",
      "chunks:2, acc:0.292, loss:3.369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:20,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3426, 0.2092, 0.0811, 0.3671, 0.0000])\n",
      "chunks:3, acc:0.287, loss:3.240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:27,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4620, 0.1299, 0.1765, 0.2315, 0.0000])\n",
      "chunks:4, acc:0.286, loss:3.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:34,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3929, 0.2165, 0.1883, 0.2024, 0.0000])\n",
      "chunks:5, acc:0.280, loss:3.091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:40,  8.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[217], line 56\u001b[0m\n\u001b[1;32m     51\u001b[0m     is_correct\u001b[38;5;241m.\u001b[39mappend(res\u001b[38;5;241m.\u001b[39margmax() \u001b[38;5;241m==\u001b[39m nuc_dict\u001b[38;5;241m.\u001b[39mget(gt,\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# we do a batched predict to process the rest of the sequence\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m predictions,seq_loss \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_of_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seq_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     loss\u001b[38;5;241m.\u001b[39mappend(seq_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[214], line 22\u001b[0m, in \u001b[0;36mpredict_on_batch\u001b[0;34m(tokenized_data, dataset, seq_idx, crop_mask_left, crop_mask_right)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     21\u001b[0m     model_outs \u001b[38;5;241m=\u001b[39m model(res,labels\u001b[38;5;241m=\u001b[39mtargets_masked)\n\u001b[0;32m---> 22\u001b[0m     fin_calculation \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fin_calculation, model_outs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 6\n",
    "predicted_prbs,gts,is_correct,loss = [],[],[],[]\n",
    "#print (dataset.iloc[0]['seq_chunked'])\n",
    "\n",
    "for no_of_index, tokenized_data in tqdm.tqdm(enumerate(data_loader)):\n",
    "    #if no_of_index < 1340:\n",
    "    #    continue\n",
    "    label = dataset.iloc[no_of_index]['seq_chunked']\n",
    "    label_len = len(label)\n",
    "    #print(no_of_index, label_len)\n",
    "    \n",
    "    # Edge case: for a sequence less then 11 nt\n",
    "    # we cannot even feed 6 mask tokens\n",
    "    # so we might as well predict random\n",
    "    if label_len < 20: \n",
    "        #print (no_of_index)\n",
    "        for i in range(label_len):\n",
    "            predicted_prbs.append(torch.tensor([0.25,0.25,0.25,0.25,0.0]))\n",
    "            gts.append(label[i])\n",
    "            is_correct.append(res.argmax().item() == nuc_dict.get(gt,4))\n",
    "        continue\n",
    "\n",
    "        \n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    tokenized_data['labels'][tokenized_data['labels']==-100] = 0\n",
    "    inputs = model_input_unaltered.clone()\n",
    "    \n",
    "\n",
    "    # First 5 nucleotides we infer from the first 6-mer\n",
    "    if MASKING:\n",
    "        inputs[:, 1:7] = 4 # we mask the first 6 6-mers\n",
    "    inputs = inputs.to(device) \n",
    "\n",
    "    targets_masked = model_input_unaltered.clone().to(device)\n",
    "    targets_masked[inputs!=4] = -100\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_outs = model(inputs,labels=targets_masked)\n",
    "        \n",
    "    model_pred = torch.softmax(model_outs['logits'], dim=2)\n",
    "    loss.append(model_outs['loss'].item())\n",
    "    \n",
    "    for i in range(5):\n",
    "        res,gt = extract_prbs_from_pred(prediction=model_pred[0],\n",
    "                                        pred_pos=1, # first 6-mer (after CLS)\n",
    "                                        token_pos=i, # we go thorugh first 6-mer\n",
    "                                        label_pos=i,\n",
    "                                        label=label)\n",
    "        predicted_prbs.append(res)\n",
    "        gts.append(gt)\n",
    "        is_correct.append(res.argmax() == nuc_dict.get(gt,4))\n",
    "\n",
    "    \n",
    "    \n",
    "    # we do a batched predict to process the rest of the sequence\n",
    "    predictions,seq_loss = predict_on_batch(tokenized_data, dataset, no_of_index)\n",
    "    if seq_loss is not None:\n",
    "        loss.append(seq_loss.item())\n",
    "    \n",
    "    # For the 6th nt up to the last 5 \n",
    "    # we extract probabilities similar to how the model was trained\n",
    "    # hiding the 4th nt of the 3rd masked 6-mer of a span of 6 masked 6-mers\n",
    "    # note that CLS makes the tokenized seq one-based\n",
    "    pos = 5 # position in sequence\n",
    "    for pos in range(5, label_len-5):\n",
    "        model_pred = predictions[pos-5]\n",
    "        res,gt = extract_prbs_from_pred(prediction=model_pred,\n",
    "                                        pred_pos=pos-2, # for i-th nt, we look at (i-2)th 6-mer\n",
    "                                        token_pos=3, # look at 4th nt in 6-mer\n",
    "                                        label_pos=pos,\n",
    "                                        label=label)\n",
    "\n",
    "        if pos==label_len//2:\n",
    "            print(res)\n",
    "            \n",
    "        predicted_prbs.append(res)\n",
    "        gts.append(gt)\n",
    "        is_correct.append(res.argmax() == nuc_dict.get(gt,4))\n",
    "\n",
    "    # Infer the last 5 nt from the last 6-mer\n",
    "    for i in range(5):\n",
    "        model_pred = predictions[pos-5]\n",
    "        res,gt = extract_prbs_from_pred(prediction=model_pred,\n",
    "                                pred_pos=pos+1, # len - 5 + 1 = last 6-mer (1-based)\n",
    "                                token_pos=i+1, # we go through last 5 of last 6-mer\n",
    "                                label_pos=pos+i,\n",
    "                                label=label)\n",
    "        predicted_prbs.append(res)\n",
    "        gts.append(gt)\n",
    "        is_correct.append(res.argmax() == nuc_dict.get(gt,4))\n",
    "    assert(len(gts) == torch.stack(predicted_prbs).shape[0]), \"{} iter, expected len:{} vs actual len:{}\".format(no_of_index,\n",
    "                                                                                   len(gts), \n",
    "                                                                     torch.stack(predicted_prbs).shape[0])\n",
    "    print(f'chunks:{no_of_index+1}, acc:{np.mean(is_correct):.3f}, loss:{np.mean(loss):.3f}')\n",
    "    #XABCDEFGHIJKL -> XABCDE [ABCDEF BCDEFG CDEFGH DEFGHI EFGHIJ FGHIJK] GHIJKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6f9d9c3e-d4fa-4532-8104-888697313c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_batch(tokenized_data, dataset, seq_idx, crop_mask_left=None, crop_mask_right=None):\n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    label = dataset.iloc[seq_idx]['seq_chunked']\n",
    "    label_len = len(label)\n",
    "    if label_len < 6:\n",
    "        return torch.ones(label_len,label_len,5)*0.25, None\n",
    "    else:\n",
    "        diag_matrix = torch.eye(tokenized_data['input_ids'].shape[1]).numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * 6, mode = 'same' ),axis = 1, arr = diag_matrix).astype(bool)\n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        masked_indices = masked_indices[3:label_len-5-2]\n",
    "        if crop_mask_left and crop_mask_right:\n",
    "                masked_indices = masked_indices[crop_mask_left:crop_mask_right]\n",
    "        res = tokenized_data['input_ids'].expand(masked_indices.shape[0],-1).clone()\n",
    "        targets_masked = res.clone().to(device)\n",
    "        res[masked_indices] = 4\n",
    "        targets_masked[res!=4] = -100\n",
    "        #print (res[0], res.shape)\n",
    "        res = res.to(device)\n",
    "        with torch.no_grad():\n",
    "            model_outs = model(res,labels=targets_masked)\n",
    "            fin_calculation = torch.softmax(model_outs['logits'], dim=2).detach().cpu()\n",
    "        return fin_calculation, model_outs['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "fce95e74-42b7-421b-a920-61b74c02a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2278, 0.1440, 0.1964, 0.4317, 0.0000])\n",
      "chunks:1, acc:0.600\n",
      "tensor([0.4078, 0.1677, 0.2584, 0.1661, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3426, 0.2092, 0.0811, 0.3671, 0.0000])\n",
      "tensor([0.4620, 0.1299, 0.1765, 0.2315, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3929, 0.2165, 0.1883, 0.2024, 0.0000])\n",
      "tensor([0.5619, 0.1091, 0.1305, 0.1985, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:01,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3746, 0.0728, 0.1724, 0.3802, 0.0000])\n",
      "tensor([0.3086, 0.1243, 0.1752, 0.3919, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:01,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4068, 0.1356, 0.1335, 0.3241, 0.0000])\n",
      "tensor([0.3109, 0.1898, 0.3150, 0.1843, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:01,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3994, 0.1980, 0.1777, 0.2249, 0.0000])\n",
      "tensor([0.3148, 0.1564, 0.1682, 0.3606, 0.0000])\n",
      "tensor([0.4001, 0.1890, 0.2355, 0.1754, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:01,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2574, 0.0989, 0.1350, 0.5087, 0.0000])\n",
      "tensor([0.3014, 0.0859, 0.1665, 0.4462, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:02,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3239, 0.2411, 0.1073, 0.3277, 0.0000])\n",
      "tensor([0.3814, 0.1694, 0.1615, 0.2876, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:02,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3899, 0.2206, 0.1039, 0.2855, 0.0000])\n",
      "tensor([0.2034, 0.1224, 0.2647, 0.4095, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:02, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1946, 0.2185, 0.2926, 0.2944, 0.0000])\n",
      "tensor([0.3070, 0.1969, 0.1983, 0.2978, 0.0000])\n",
      "tensor([0.1478, 0.2628, 0.3095, 0.2799, 0.0000])\n",
      "tensor([0.2770, 0.2449, 0.1380, 0.3401, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:02, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2930, 0.3015, 0.0708, 0.3348, 0.0000])\n",
      "tensor([0.2459, 0.1081, 0.2468, 0.3992, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:03,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1350, 0.2423, 0.1678, 0.4550, 0.0000])\n",
      "tensor([0.2691, 0.2508, 0.2146, 0.2655, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [00:03,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5871, 0.1030, 0.1449, 0.1650, 0.0000])\n",
      "tensor([0.3560, 0.1648, 0.1598, 0.3194, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:03,  9.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3616, 0.0876, 0.1929, 0.3579, 0.0000])\n",
      "tensor([0.2667, 0.2315, 0.1466, 0.3552, 0.0000])\n",
      "tensor([0.2394, 0.2197, 0.2845, 0.2564, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:04,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1977, 0.1769, 0.2561, 0.3693, 0.0000])\n",
      "tensor([0.5236, 0.1542, 0.1440, 0.1781, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:04,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1961, 0.2368, 0.2363, 0.3307, 0.0000])\n",
      "tensor([0.2626, 0.1760, 0.3978, 0.1637, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:04,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2358, 0.3202, 0.1840, 0.2600, 0.0000])\n",
      "tensor([0.2969, 0.2292, 0.1631, 0.3107, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:04,  7.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2034, 0.2486, 0.3277, 0.2203, 0.0000])\n",
      "tensor([0.2872, 0.2571, 0.1601, 0.2956, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:05,  7.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[221], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m token_idx_left,token_idx_right \u001b[38;5;241m=\u001b[39m central_token_idx,central_token_idx\u001b[38;5;241m+\u001b[39mcentral_window\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m6\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m pos_left,pos_right \u001b[38;5;241m=\u001b[39m central_pos, central_pos\u001b[38;5;241m+\u001b[39mcentral_window\n\u001b[0;32m---> 23\u001b[0m predictions,seq_loss \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_of_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mcrop_mask_left\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_idx_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcrop_mask_right\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_idx_right\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pos_left):\n\u001b[1;32m     27\u001b[0m     predicted_prbs\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m0.25\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[214], line 22\u001b[0m, in \u001b[0;36mpredict_on_batch\u001b[0;34m(tokenized_data, dataset, seq_idx, crop_mask_left, crop_mask_right)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     21\u001b[0m     model_outs \u001b[38;5;241m=\u001b[39m model(res,labels\u001b[38;5;241m=\u001b[39mtargets_masked)\n\u001b[0;32m---> 22\u001b[0m     fin_calculation \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fin_calculation, model_outs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "central_window =5\n",
    "\n",
    "k = 6\n",
    "predicted_prbs,gts,is_correct = [],[],[]\n",
    "\n",
    "for no_of_index, tokenized_data in tqdm.tqdm(enumerate(data_loader)):\n",
    "\n",
    "    label = dataset.iloc[no_of_index]['seq_chunked']\n",
    "    L = len(label)\n",
    "\n",
    "    if L < 20: \n",
    "        #print (no_of_index)\n",
    "        for i in range(L):\n",
    "            predicted_prbs.append(torch.tensor([0.25,0.25,0.25,0.25,0.25]))\n",
    "            gts.append(label[i])\n",
    "        continue\n",
    "        \n",
    "    central_pos = L//2\n",
    "    central_token_idx = central_pos-5 #first token, where the central nucleotide appears\n",
    "    token_idx_left,token_idx_right = central_token_idx,central_token_idx+central_window+6-1\n",
    "    pos_left,pos_right = central_pos, central_pos+central_window\n",
    "\n",
    "    predictions,seq_loss = predict_on_batch(tokenized_data, dataset, no_of_index,\n",
    "                                           crop_mask_left=token_idx_left,crop_mask_right=token_idx_right)\n",
    "\n",
    "    for pos in range(pos_left):\n",
    "        predicted_prbs.append(torch.tensor([0.25,0.25,0.25,0.25,0.25]))\n",
    "        gts.append(label[pos])\n",
    "        \n",
    "    for pos in range(pos_left,pos_right):\n",
    "            model_pred = predictions[pos-pos_left]\n",
    "            res,gt = extract_prbs_from_pred(prediction=model_pred,\n",
    "                                            pred_pos=pos-2, # for i-th nt, we look at (i-2)th 6-mer\n",
    "                                            token_pos=3, # look at 4th nt in 6-mer\n",
    "                                            label_pos=pos,\n",
    "                                             label=label)                \n",
    "            if pos==L//2:\n",
    "                print(res)\n",
    "            predicted_prbs.append(res)\n",
    "            gts.append(gt)\n",
    "            is_correct.append(res.argmax() == nuc_dict.get(gt,4))\n",
    "    \n",
    "    for pos in range(pos_right,L):\n",
    "        predicted_prbs.append(torch.tensor([0.25,0.25,0.25,0.25,0.25]))\n",
    "        gts.append(label[pos])\n",
    "\n",
    "    if no_of_index%100==0:\n",
    "        print(f'chunks:{no_of_index+1}, acc:{np.mean(is_correct):.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1cf24fdc-1581-453c-bb79-238449661e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f73358d9-a7da-4c3a-a0a5-3c5517a52270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3638e0e5-8531-4a57-9c2c-a23f2c5c2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "053ad364-d887-47a9-bdde-2ef6af76fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    label = dataset.iloc[no_of_index]['seq_chunked']\n",
    "    label_len = len(label)\n",
    "    if label_len < 6:\n",
    "        pass\n",
    "    #    return torch.zeros(label_len,label_len,5), None\n",
    "    else:\n",
    "        if MASKING:\n",
    "            diag_matrix = torch.eye(tokenized_data['input_ids'].shape[1]).numpy()\n",
    "            masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * 6, mode = 'same' ),axis = 1, arr = diag_matrix).astype(bool)\n",
    "            masked_indices = torch.from_numpy(masked_indices)\n",
    "            #masked_indices = masked_indices[3:label_len-5-2]\n",
    "            masked_indices = masked_indices[]\n",
    "            res = tokenized_data['input_ids'].expand(masked_indices.shape[0],-1).clone()\n",
    "            targets_masked = res.clone().to(device)\n",
    "            res[masked_indices] = 4\n",
    "            targets_masked[res!=4] = -100\n",
    "            #print (res[0], res.shape)\n",
    "        res = res.to(device)\n",
    "        with torch.no_grad():\n",
    "            model_outs = model(res,labels=targets_masked)\n",
    "            fin_calculation = torch.softmax(model_outs['logits'], dim=2).detach().cpu()   \n",
    "        #return fin_calculation, model_outs['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "05fcbfe5-39d6-404d-bd8a-02d6ad0c11e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,    4,    4,    4,    4,    4,    4, 1130,  410, 1628, 2403, 1406,\n",
       "        1515, 1952, 3698, 2490, 1753, 2902, 3401, 1301, 1094,  267, 1054,  105,\n",
       "         406, 1609, 2326, 1099,  286, 1130,  410, 1626, 2394, 1369, 1367, 1357,\n",
       "        1319, 1165,  550, 2186,  537, 2136,  339, 1344, 1265,  951, 3789, 2856,\n",
       "        3219,  574, 2281,  920, 3665, 2358, 1226,  794, 3161,  343, 1358, 1321,\n",
       "        1173,  582, 2314, 1051,   95,  365, 1446, 1674, 2587, 2142,  361, 1430,\n",
       "        1610, 2329, 1110,  330, 1305, 1111,  333, 1319, 1166,  556, 2209,  629,\n",
       "        2503, 1807, 3117,  166,  650, 2586, 2139,  349, 1384, 1426, 1596, 2275,\n",
       "         894, 3562, 1946, 3674, 2395, 1376, 1395, 1472, 1777, 3000, 3795, 2879,\n",
       "        3309,  933, 3720, 2579, 2112,  242,  956, 3810, 2938, 3545, 1877, 3400,\n",
       "        1298, 1081,  215,  845, 3365, 1158,  523, 2077,  102,  393, 1558, 2121,\n",
       "         277, 1095,  272, 1073,  181,  712, 2835, 3136,  242,  954, 3802, 2908,\n",
       "        3425, 1397, 1480, 1809, 3126,  201,  791, 3150,  299, 1182,  620, 2468,\n",
       "        1666, 2554, 2009, 3926, 3403, 1309, 1127,  400, 1586, 2233,  726, 2891,\n",
       "        3357, 1125,  389, 1542, 2060,   33,  117,  454, 1801, 3096,   82,  313,\n",
       "        1240,  851, 3390, 1260,  929, 3702, 2506, 1820, 3170,  377, 1493, 1863,\n",
       "        3344, 1074,  188,  740, 2945, 3576, 2001, 3896, 3283,  832, 3314,  956,\n",
       "        3811, 2942, 3563, 1951, 3695, 2478, 1705, 2709, 2629, 2312, 1041,   53,\n",
       "         197,  776, 3092,   66,  251,  992, 3955, 3517, 1766, 2955, 3616, 2161,\n",
       "         440, 1746, 2873, 3286,  844, 3361, 1143,  463, 1838, 3243,  672, 2676,\n",
       "        2498, 1785, 3031, 3918, 3369, 1174,  586, 2332, 1122,  377, 1495, 1871,\n",
       "        3375, 1199,  686, 2730, 2714, 2650, 2394, 1369, 1367, 1358, 1321, 1173,\n",
       "         582, 2314, 1051,   93,  359, 1422, 1577, 2199,  592, 2353, 1207,  720,\n",
       "        2868, 3266,  763, 3039, 3949, 3495, 1678, 2601, 2198,  586, 2332, 1121,\n",
       "         373, 1477, 1800, 3089,   54,  204,  801, 3189,  454, 1802, 3098,   91,\n",
       "         349, 1383, 1424, 1586, 2233,  728, 2897, 3381, 1222,  780, 3108,  130,\n",
       "         505, 2008, 3924, 3395, 1279, 1008, 4017, 3768, 2770, 2873, 3288,  849,\n",
       "        3382, 1225,  789, 3141,  264, 1044,   65,  245,  966, 3849, 3093,   70,\n",
       "         265, 1045,   71,  271, 1069,  168,  659, 2622, 2281,  917, 3654, 2314,\n",
       "        1050,   90,  346, 1370, 1371, 1373, 1382, 1420, 1570, 2172,  481, 1911,\n",
       "        3534, 1834, 3226,  601, 2392, 1362, 1340, 1251,  895, 3568, 1969, 3766,\n",
       "        2762, 2843, 3166,  363, 1438, 1643, 2463, 1645, 2470, 1673, 2583, 2125,\n",
       "         294, 1164,  545, 2167,  464, 1841, 3253,  712, 2833, 3126,  201,  791,\n",
       "        3152,  305, 1205,  710, 2828, 3105,  118,  457, 1814, 3148,  289, 1142,\n",
       "         459, 1822, 3179,  414, 1641, 2454, 1612, 2339, 1149,  486, 1930, 3610,\n",
       "        2137,  342, 1354, 1306, 1113,  342, 1353, 1301, 1093,  263, 1037,   39,\n",
       "         141,  550, 2186,  539, 2143,  367, 1453, 1704, 2705, 2616, 2260,  836,\n",
       "        3330, 1017, 4054, 3913, 3351, 1103,  302, 1193,  661, 2632, 2323, 1086,\n",
       "         233,  920, 3665, 2359, 1230,  810, 3225,  597, 2373, 1286, 1034,   25,\n",
       "          86,  329, 1301, 1096,  273, 1077,  200,  786, 3129,  216,  849, 3384,\n",
       "        1235,  830, 3306,  921, 3670, 2380, 1314, 1147,  478, 1900, 3490, 1657,\n",
       "        2519, 1871, 3374, 1196,  673, 2677, 2501, 1798, 3084,   33,  120,  466,\n",
       "        1852, 3298,    3,    0,    0,    0], device='cuda:0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ae2d2-7580-40c3-90c7-673c4e889283",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8cfc06-e87f-4144-abc6-35a44254c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prbs = np.array(predicted_prbs)[:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d594e75-1254-4b05-9490-4263dba58f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['seq_name','original_seq']].drop_duplicates()\n",
    "dataset['seq_len'] = dataset.original_seq.apply(len)\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "s = 0\n",
    "\n",
    "for seq_name, original_seq, seq_len in dataset.values.tolist():\n",
    "    seq_probas = predicted_prbs[s:s+seq_len,:]\n",
    "    s += seq_len\n",
    "    if strand_info[seq_name]=='-' and dna_model:\n",
    "        seq_probas = seq_probas[::-1,[3,2,1,0]] #reverse complement probabilities s.t. probas match original_seq\n",
    "    all_preds.append((seq_name,original_seq, seq_probas))\n",
    "\n",
    "with open(output_dir + f\"/predictions_{fold}.pickle\", \"wb\") as f:\n",
    "    seq_names, seqs, probs = zip(*all_preds)\n",
    "    pickle.dump({'seq_names':seq_names, 'seqs':seqs, 'probs':probs, 'fasta':fasta},f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564eb1b9-1249-464a-ad28-4de86b1feb43",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24320f1c-c55c-4a64-942c-aa9f24307cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.max(prbs_arr,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896b80c-0362-460e-9cfc-013689861d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "np.sum(gts == np.array([\"A\",\"C\",\"G\",\"T\"])[np.argmax(prbs_arr,axis=1)])/len(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25411478-cf13-4ecc-bcaf-bc9591af8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nt in [\"A\", \"C\", \"G\", \"T\"]:\n",
    "    nt_arr = np.array([nt]*len(gts))\n",
    "    actual = np.sum(gts == nt_arr)/len(gts)\n",
    "    predicted = np.sum(np.array([\"A\",\"C\",\"G\",\"T\"])[np.argmax(prbs_arr,axis=1)] == nt_arr)/len(gts)\n",
    "    print(\"{}: Actual {}, Predicted {}\".format(nt, actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42eec2b-ec52-4142-86f2-025b4387953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prbs = torch.log(torch.stack(predicted_prbs)[:,:-1])\n",
    "class_labels = torch.tensor(class_label_gts(gts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7e2e8-b962-4f14-82d4-4d512a7b11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.nll_loss(log_prbs, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c2aa84-aa6d-418c-beeb-d64fcbdbe56e",
   "metadata": {},
   "source": [
    "# Make data fit metrics handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa217b54-a115-4c44-bd8b-af28858fd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_path = \"outputs/gpar_bertadn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5821776e-d73e-48fe-9517-a70d8fc86b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get targets\n",
    "targets = torch.tensor(class_label_gts(gts))\n",
    "stacked_prbs = torch.stack(predicted_prbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ac2f0-f18b-42cd-8ef3-a0f145bd32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cross entropy, it's already as probability so just nll\n",
    "ce = torch.nn.functional.nll_loss(stacked_prbs, targets, reduction=\"none\") #cross_entropy(prbs, targets)\n",
    "\n",
    "#print(ce)\n",
    "\n",
    "# save\n",
    "torch.save(stacked_prbs,  out_path+\"masked_logits.pt\") # no logits, so use prbs\n",
    "torch.save(torch.argmax(stacked_prbs, dim=1),  out_path+\"masked_preds.pt\")\n",
    "torch.save(stacked_prbs,  out_path+\"prbs.pt\")\n",
    "torch.save(ce, out_path+\"ce.pt\")\n",
    "\n",
    "# save targets\n",
    "torch.save(targets, out_path+\"masked_targets.pt\")\n",
    "\n",
    "# save rest as placeholders (zeros of same length)\n",
    "torch.save(torch.zeros(len(stacked_prbs)), out_path+\"masked_motifs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6acad-bbbd-47c1-81bd-4fd9a5e5b7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
